{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardTanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=-1.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(5, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(5, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer0): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ResNet(ResidualBlock, [3, 4, 6, 3], num_classes = 10).to(\"cpu\")\n",
    "model2.load_state_dict(torch.load(\"best_resnet50_MINST-DVS_Hardtanh_ReLUmaxpool.pt\", weights_only=True))\n",
    "model2.to(\"cpu\")\n",
    "model2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na ISNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_spiking(tj, W, D_i, t_min, t_max, noise, dtype=torch.FloatTensor):\n",
    "    \"\"\"\n",
    "    Calculates spiking times to recover ReLU-like functionality.\n",
    "    Assumes tau_c=1 and B_i^(n)=1.\n",
    "    \"\"\"\n",
    "    # Calculate the spiking threshold (Eq. 18)\n",
    "    threshold = t_max - t_min - D_i\n",
    "    \n",
    "    # Calculate output spiking time ti (Eq. 7)\n",
    "\n",
    "    ti = torch.matmul((tj - t_min).type(dtype), W.type(dtype)) + threshold + t_min\n",
    "    \n",
    "    # Ensure valid spiking time: do not spike for ti >= t_max\n",
    "    ti = torch.where(ti < t_max, ti, t_max)\n",
    "\n",
    "    # Add noise to the spiking time for noise simulations\n",
    "    if noise > 0:\n",
    "        ti = ti + torch.randn_like(ti) * noise\n",
    "    \n",
    "    return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingDense(nn.Module):\n",
    "    def __init__(self, units, name, X_n=1, outputLayer=False, robustness_params={}, input_dim=None,\n",
    "                 kernel_regularizer=None, kernel_initializer=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.B_n = (1 + 0.0) * X_n\n",
    "        self.outputLayer=outputLayer\n",
    "        self.t_min_prev, self.t_min, self.t_max=0, 0, 1\n",
    "        self.noise=robustness_params['noise']\n",
    "        self.time_bits=robustness_params['time_bits']\n",
    "        self.weight_bits =robustness_params['weight_bits'] \n",
    "        self.w_min, self.w_max=-1.0, 1.0\n",
    "        self.alpha = torch.full((units,), 1, dtype=torch.float64)\n",
    "        self.input_dim=input_dim\n",
    "        self.regularizer = kernel_regularizer\n",
    "        self.initializer = kernel_initializer\n",
    "        self.bias = False\n",
    "    \n",
    "    def build(self, input_dim, kernel : torch.Tensor = None, bias : torch.Tensor = None):\n",
    "        # Ensure input_dim is defined properly if not passed.\n",
    "        if input_dim[-1] is None:\n",
    "            input_dim = (None, self.input_dim)\n",
    "        else:\n",
    "            self.input_dim = input_dim\n",
    "        # Create kernel weights and D_i.\n",
    "        if kernel is not None:\n",
    "            if bias is None:\n",
    "                self.kernel = nn.Parameter(kernel.clone())\n",
    "            else:\n",
    "                self.kernel = nn.Parameter(torch.concat((kernel.clone(),bias.clone().unsqueeze(0))))\n",
    "                self.bias = True\n",
    "        else:\n",
    "            self.kernel = nn.Parameter(torch.empty(input_dim[-1], self.units))\n",
    "        self.D_i = nn.Parameter(torch.zeros(self.units))\n",
    "\n",
    "        # Apply the initializer if provided.\n",
    "        if self.initializer:\n",
    "            self.kernel = self.initializer(self.kernel) # tu zmiana TODO\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max parameters of this layer. Alpha is fixed at 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias:\n",
    "            max_W = torch.concat((torch.maximum(self.kernel[:-1],torch.zeros(self.kernel[:-1].shape)), self.kernel[-1].unsqueeze(0)))\n",
    "            max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
    "        else:\n",
    "            max_input = torch.tensor(in_ranges_max)\n",
    "            max_W = torch.maximum(self.kernel,torch.zeros(self.kernel.shape))\n",
    "        output_val = F.relu(torch.matmul(max_input,max_W))\n",
    "        max_V = F.relu(torch.max(torch.matmul(max_input,max_W)))\n",
    "\n",
    "        self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "        \n",
    "        # Returning for function signature consistency\n",
    "        return t_min, t_min + self.B_n*max_V, output_val\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times `tj`, output spiking times `ti` or membrane potential value for the output layer.\n",
    "        \"\"\"\n",
    "        # Call the custom spiking logic\n",
    "        if self.bias:\n",
    "            print(tj.shape)\n",
    "            new_tj = torch.concat((tj, torch.tensor([[(self.t_min - 1)]])), dim=1)\n",
    "            output = call_spiking(new_tj, self.kernel, self.D_i, self.t_min, self.t_max, noise=self.noise)\n",
    "        else:\n",
    "            output = call_spiking(tj, self.kernel, self.D_i, self.t_min, self.t_max, noise=self.noise)\n",
    "        # If this is the output layer, perform the special integration logic\n",
    "        if self.outputLayer:\n",
    "            # Compute weighted product\n",
    "            W_mult_x = torch.matmul(self.t_min - tj, self.kernel)\n",
    "            self.alpha = self.D_i / (self.t_min - self.t_min_prev)\n",
    "            output = self.alpha * (self.t_min - self.t_min_prev) + W_mult_x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingConv2D(nn.Module):\n",
    "    def __init__(self, filters, name, X_n=1, padding='same', kernel_size=(3,3), robustness_params=None, kernels = None, device = 'cuda:0', biases = None, stride=1):\n",
    "        super(SpikingConv2D, self).__init__()\n",
    "        self.stride = stride\n",
    "        if robustness_params is None:\n",
    "            robustness_params = {}\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.B_n = (1 + 0.0) * X_n\n",
    "        self.t_min_prev, self.t_min, self.t_max = 0, 0, 1\n",
    "        self.w_min, self.w_max = -1.0, 1.0\n",
    "        self.time_bits = robustness_params.get('time_bits', 1)\n",
    "        self.weight_bits = robustness_params.get('weight_bits', 1) \n",
    "        self.noise = robustness_params.get('noise', 0.0)\n",
    "        self.device = device\n",
    "        # Initialize alpha as a tensor of ones\n",
    "        self.alpha = nn.Parameter(torch.ones(filters, dtype=torch.float32))\n",
    "        \n",
    "        # Registering the kernel as a learnable parameter\n",
    "        #TODO:\n",
    "        if kernels is not None:\n",
    "            self.kernel = nn.Parameter(kernels).to(device)\n",
    "        else:\n",
    "            self.kernel = nn.Parameter(torch.randn(filters, 1, kernel_size[0], kernel_size[1], dtype=torch.float32)).to(device)\n",
    "        if biases is not None:\n",
    "            self.B = biases.unsqueeze(1).to(self.device)\n",
    "        else:\n",
    "            self.B = nn.Parameter(torch.zeros(filters, 1, dtype=torch.float32)).to(self.device)\n",
    "\n",
    "        # Placeholder for batch normalization parameters\n",
    "        self.BN = nn.Parameter(torch.tensor([0], dtype=torch.float32), requires_grad=False)\n",
    "        self.BN_before_ReLU = nn.Parameter(torch.tensor([0], dtype=torch.float32), requires_grad=False)\n",
    "        \n",
    "        # Parameter for different thresholds\n",
    "        self.D_i = nn.Parameter(torch.zeros(9, filters, dtype=torch.float32)).to(self.device)\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        max_W = torch.maximum(self.kernel,torch.zeros(self.kernel.shape).to(self.device))\n",
    "        # print(max_W.shape)\n",
    "        \n",
    "        max_input = (in_ranges_max.unsqueeze(-1).unsqueeze(-1)).to(self.device) * torch.ones(self.kernel.shape[1:]).to(self.device)\n",
    "\n",
    "        # print(max_input.shape)\n",
    "        if self.B is not None:\n",
    "            max_V = F.relu(torch.max(torch.sum(torch.mul(max_input,max_W),(1,2,3))+self.B.squeeze(1)))\n",
    "            max_values = F.relu(torch.sum(torch.mul(max_input,max_W),(1,2,3))+self.B.squeeze(1))\n",
    "        else:\n",
    "            max_V = F.relu(torch.max(torch.sum(torch.mul(max_input,max_W),(1,2,3))))\n",
    "            max_values = F.relu(torch.sum(torch.mul(max_input,max_W),(1,2,3)))\n",
    "        self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        \n",
    "        # Returning for function signature consistency\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), max_values\n",
    "\n",
    "    def call_spiking(self, tj, W, D_i, t_min, t_max, noise):\n",
    "        \"\"\"\n",
    "        Calculates spiking times from which ReLU functionality can be recovered.\n",
    "        \"\"\"\n",
    "        threshold = t_max - t_min - D_i\n",
    "        \n",
    "        # Calculate output spiking time ti\n",
    "        ti = torch.matmul(tj - t_min, W) + threshold + t_min\n",
    "        \n",
    "        # Ensure valid spiking time\n",
    "        ti = torch.where(ti < t_max, ti, t_max)\n",
    "        \n",
    "        # Add noise\n",
    "        if noise > 0:\n",
    "            ti += torch.randn_like(ti) * noise\n",
    "        \n",
    "        return ti\n",
    "\n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times tj, output spiking times ti. \n",
    "        \"\"\"\n",
    "        if self.stride==1:\n",
    "            padding_size = int(self.padding == 'same') * ((self.kernel_size[0]-1) // 2)\n",
    "        else:\n",
    "            # dont know if it works with stride other than 1 always set padding to valid\n",
    "            padding_size = int(self.padding == 'same') * ((self.kernel_size[0]-1) // 2)\n",
    "        image_same_size = tj.size(2) \n",
    "        image_valid_size = image_same_size - self.kernel_size[0] + 1\n",
    "\n",
    "\n",
    "        tj_shape = tj.shape\n",
    "        # Dodanie paddingu\n",
    "        if self.padding == 'same':\n",
    "            tj = torch.nn.functional.pad(tj, (padding_size, padding_size, padding_size, padding_size), value=self.t_min)\n",
    "        elif type(self.padding) is tuple:\n",
    "            tj = torch.nn.functional.pad(tj, (self.padding[0], self.padding[0], self.padding[1], self.padding[1]), value=self.t_min)\n",
    "            pass\n",
    "        # Wyciąganie patchy\n",
    "        if self.stride==1:\n",
    "            batch_size, in_channels, input_height, input_width = tj.shape\n",
    "            tj = torch.nn.functional.unfold(tj, kernel_size=self.kernel_size, stride=1).transpose(1, 2)\n",
    "            # Reshape dla wag\n",
    "            W = self.kernel.view(self.filters, -1).t()\n",
    "            out_channels, _, kernel_height, kernel_width = self.kernel.shape\n",
    "            output_height = (input_height - kernel_height) // self.stride + 1\n",
    "            output_width = (input_width - kernel_width) // self.stride + 1\n",
    "        else:\n",
    "            batch_size, in_channels, input_height, input_width = tj.shape\n",
    "            tj = torch.nn.functional.unfold(tj, kernel_size=self.kernel_size, stride=self.stride).transpose(1, 2)\n",
    "            out_channels, _, kernel_height, kernel_width = self.kernel.shape\n",
    "            output_height = (input_height - kernel_height) // self.stride + 1\n",
    "            output_width = (input_width - kernel_width) // self.stride + 1\n",
    "            # Reshape dla wag\n",
    "            W = self.kernel.view(out_channels, -1).t()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (self.padding == 'valid' or self.BN != 1 or self.BN_before_ReLU == 1) and (self.B is None): \n",
    "\n",
    "            ti = self.call_spiking(tj, W, self.D_i[0], self.t_min, self.t_max, noise=self.noise).transpose(1, 2)\n",
    "            if self.padding == 'valid':\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "            else:\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "        elif self.B is not None:\n",
    "            ## concatenating simple \"one\" to vector of times\n",
    "            one_as_time = self.t_min - 1\n",
    "            tj = torch.concat((tj, one_as_time * torch.ones(tj.shape[0],tj.shape[1],1).to(self.device)), 2)\n",
    "            ## conttenating biases to weight vector\n",
    "            W = torch.concat((W,self.B.T),0)\n",
    "            ti = self.call_spiking(tj, W, self.D_i[0], self.t_min, self.t_max, noise=self.noise).transpose(1, 2)\n",
    "            if self.padding == 'valid':\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "            else:\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "        return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_and_bn(conv, bn, device = 'cuda:0'):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv2d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\twith torch.no_grad():\n",
    "\t\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) ).to(device)\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\twith torch.no_grad():\n",
    "\t\tfusedconv.bias.copy_( (torch.matmul(w_bn, b_conv) + b_bn) )\n",
    "\t\n",
    "\treturn fusedconv.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaxMinPool2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Max Pooling or Min Pooling operation, depending on the sign of the batch normalization layer before.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, max_time, stride=None, padding=0, dilation=1):\n",
    "        super(MaxMinPool2D, self).__init__()\n",
    "        \n",
    "        # Default sign is 1, indicating max pooling functionality.\n",
    "        self.sign = nn.Parameter(-1*torch.ones(1, 1, 1, 1), requires_grad=False)\n",
    "        self.dilation = dilation\n",
    "        # MaxPool2d setup (will be used in call)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.max_time = max_time\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Applying the sign to the inputs (if sign is -1, it will act as Min Pooling)\n",
    "        padding_size = self.padding\n",
    "        inputs = torch.nn.functional.pad(inputs, (padding_size, padding_size, padding_size, padding_size), value=self.max_time)\n",
    "        pooled = F.max_pool2d(self.sign * inputs, kernel_size=self.kernel_size, stride=self.stride, padding=0, dilation=self.dilation)\n",
    "        \n",
    "        # Multiply the pooled result by the sign, which controls the pooling type\n",
    "        return pooled * self.sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddSNNLayer, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        output_val = input1_val + input2_val\n",
    "        max_V = max(output_val)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = tj1 + tj2 - 2*self.t_min  + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubSNNLayer, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        output_val = input1_val\n",
    "        max_V = max(output_val)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = tj1 - self.t_min - (tj2 - self.t_min)  + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentitySNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentitySNNLayer, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "\n",
    "        max_input = max(in_ranges_max)\n",
    "        max_V = max_input\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), in_ranges_max\n",
    "\n",
    "    def forward(self, tj):\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = tj - self.t_min  + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualBlockSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSNNBlock(nn.Module):\n",
    "    def __init__(self,resblock : ResidualBlock, in_channels, out_channels, stride=1, downsample=None, robustness_params = None, device = \"cuda:0\"):\n",
    "        super(ResidualSNNBlock, self).__init__()\n",
    "        conv = resblock.conv1[0]\n",
    "        bn= resblock.conv1[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        self.conv1 = SpikingConv2D(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        self.device = device\n",
    "        \n",
    "        conv = resblock.conv2[0]\n",
    "        bn= resblock.conv2[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        self.conv2 = SpikingConv2D(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.identity = IdentitySNNLayer()\n",
    "        self.add_layer = AddSNNLayer()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input_val, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        t_min1, t_max1, conv1_val = self.conv1.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "        self.pooling1 = MaxMinPool2D(2, t_max1.data,2).to(self.device)\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, in_ranges_max=conv1_val)\n",
    "        max_out2 = t_max2 - t_min2\n",
    "        \n",
    "        if self.downsample:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "            self.pooling2 = MaxMinPool2D(2, t_max1_dummy.data,2).to(self.device)\n",
    "        else:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, minimal_t_max=t_max1_dummy, in_ranges_max=conv1_val)\n",
    "        \n",
    "        # time t_max2 and t_max1_dummy are the same\n",
    "        t_min_add = t_max2 - max(max_dummy1, max_out2)\n",
    "\n",
    "        self.t_min, self.t_max, add_val = self.add_layer.set_params(t_min_add, t_max2, conv2_val, downsample_val)\n",
    "\n",
    "        self.times = [(t_min1, t_max1, 'c'), (t_min2, t_max2, 'c'), (self.t_min, self.t_max, 'a') ]\n",
    "        return self.t_min, self.t_max, add_val\n",
    "\n",
    "    def get_main_times(self):\n",
    "        return self.times\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.pooling2(residual)\n",
    "            out = self.pooling1(out)\n",
    "        else:\n",
    "            residual = self.identity(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.add_layer(out,residual)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSNN(nn.Module):\n",
    "    def __init__(self, layer, inplanes, planes, blocks, stride=1, device = 'cuda:0'):\n",
    "        self.inplanes = inplanes\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            conv2d, bias_from_nn = layer[0].downsample[0], layer[0].downsample[1]\n",
    "            conv_fused = fuse_conv_and_bn(conv2d, bias_from_nn)\n",
    "            robustness_params={\n",
    "                'noise':0.0,\n",
    "                'time_bits':0,\n",
    "                'weight_bits': 0,\n",
    "                'latency_quantiles':0.0\n",
    "            }\n",
    "            downsample = SpikingConv2D(planes,\"test2\", padding='same', stride=1, device=device,robustness_params=robustness_params,kernels=conv_fused.weight.data, biases=conv_fused.bias, kernel_size=(1,1))\n",
    "            # t_min, t_max = spiking_conv2.set_params(0,1)stride\n",
    "            \n",
    "        self.layers = []\n",
    "        self.layers.append(ResidualSNNBlock(layer[0],self.inplanes,planes, 1, downsample=downsample, device=device))\n",
    "        self.inplanes = planes\n",
    "        self.blocks = blocks\n",
    "        for i in range(1, blocks):\n",
    "            self.layers.append(ResidualSNNBlock(layer[i],self.inplanes,planes, 1, downsample=None, device=device))\n",
    "        \n",
    "    \n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        tmin, tmax = t_min_prev, t_min\n",
    "        for i in range(self.blocks):\n",
    "            tmin, tmax, in_ranges_max = self.layers[i].set_params(tmin, tmax,in_ranges_max)\n",
    "        return tmin, tmax, in_ranges_max\n",
    "    \n",
    "    def get_main_times(self):\n",
    "        lst = []\n",
    "        for i in range(self.blocks):\n",
    "            lst.extend(self.layers[i].get_main_times())\n",
    "        return lst\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.blocks):\n",
    "            x = self.layers[i].forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.rand(1, 5, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1 = model2.conv1(random_input)\n",
    "model_maxpool = model2.maxpool(model_conv1)\n",
    "model_resblock0 = model2.layer0[0](model_maxpool)\n",
    "model_resblock1 = model2.layer0[1](model_resblock0)\n",
    "model_resblock2 = model2.layer0[2](model_resblock1)\n",
    "model_layer0 = model2.layer0(model_maxpool)\n",
    "model_layer1 = model2.layer1(model_layer0)\n",
    "model_layer2 = model2.layer2(model_layer1)\n",
    "model_layer3 = model2.layer3(model_layer2)\n",
    "model_maxpool2 = model2.avgpool(model_layer3)\n",
    "# model2.fc.bias = nn.Parameter(torch.ones(10)*1000)\n",
    "model_linear = F.relu(model2.fc(model_maxpool2.view(model_layer3.size(0), -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingConv2D_Htanh(nn.Module):\n",
    "    def __init__(self, filters, name, X_n=1, padding='same', kernel_size=(3,3), robustness_params=None, kernels = None, device = 'cuda:0', biases = None, stride=1):\n",
    "        super(SpikingConv2D_Htanh, self).__init__()\n",
    "        if robustness_params is None:\n",
    "            robustness_params = {}\n",
    "        kernels_pos = torch.concat((kernels,-kernels), dim=1 )\n",
    "        if biases is not None:\n",
    "            biases_pos = biases\n",
    "        else:\n",
    "            biases_pos = None\n",
    "        \n",
    "        kernels_neg = torch.concat((-kernels,kernels), dim=1 )\n",
    "        if biases is not None:\n",
    "            biases_neg = -biases\n",
    "        else:\n",
    "            biases_neg = None\n",
    "\n",
    "        kernels_new = torch.concat((kernels_pos, kernels_neg), dim=0)\n",
    "        biases_new = torch.concat((biases_pos, biases_neg))\n",
    "        print(biases_new.shape)\n",
    "        self.conv_first = SpikingConv2D(2*filters, name, device = device, padding=padding, stride=stride, kernel_size=kernel_size,robustness_params=robustness_params, kernels=kernels_new, biases=biases_new)\n",
    "        \n",
    "        kernels_new2 = torch.concat((kernels_pos, kernels_neg), dim=0)\n",
    "        biases_new2 = torch.concat((biases_pos, biases_neg)) - 1\n",
    "        self.conv_second = SpikingConv2D(2*filters, name, device = device, padding=padding, stride=stride, kernel_size=kernel_size,robustness_params=robustness_params, kernels=kernels_new2, biases=biases_new2)\n",
    "        self.sub = SubSNNLayer()\n",
    "        self.filters = filters*2\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        if(in_ranges_max.shape[0] != self.filters):\n",
    "            in_ranges_max = torch.concat((in_ranges_max,torch.zeros(in_ranges_max.shape)))\n",
    "        tmin1, tmax1, first_val = self.conv_first.set_params(t_min_prev, t_min, in_ranges_max, minimal_t_max-1)\n",
    "        tmin2, tmax2, second_val = self.conv_second.set_params(t_min_prev, t_min, in_ranges_max, tmax1)\n",
    "\n",
    "        tmin1, tmax1, first_val = self.conv_first.set_params(t_min_prev, t_min, in_ranges_max, tmax2)\n",
    "        self.t_max = tmax1\n",
    "        tmins, tmaxs, sub_val = self.sub.set_params(t_min, tmax1, first_val,second_val)\n",
    "        tmaxs = tmins+1\n",
    "        self.sub.t_max = tmaxs\n",
    "        self.t_max = tmaxs\n",
    "        # Returning for function signature consistency\n",
    "        return tmins, self.t_max, torch.minimum(sub_val, torch.ones(sub_val.shape))\n",
    "        # return tmin1, self.t_max, torch.minimum(first_val, torch.ones(first_val.shape))\n",
    "\n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times tj, output spiking times ti. \n",
    "        \"\"\"\n",
    "        tj1 = self.conv_first(tj)\n",
    "        tj2 = self.conv_second(tj)\n",
    "        tj_sub = self.sub(tj1, tj2)\n",
    "\n",
    "        return tj_sub\n",
    "        # return tj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSNNLayer_all(nn.Module):\n",
    "    def __init__(self, bias=0):\n",
    "        super(AddSNNLayer_all, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        self.B = bias # bias for all inputs (for Hard tanh)\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        if input2_val.shape[0] != input1_val.shape[0]:\n",
    "            input2_val = torch.concat((input2_val, torch.zeros(input2_val.shape)))\n",
    "        output_val = input1_val + input2_val + self.B\n",
    "        max_V = max(output_val)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "\n",
    "        self.channels = tj1.shape[1]//2\n",
    "\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = torch.concat((tj1[0, :self.channels] + tj2[0, :self.channels] - tj1[0, self.channels:] - tj2[0, self.channels:], \n",
    "                           tj1[0, self.channels:] + tj2[0, self.channels:] - tj1[0, :self.channels] - tj2[0, :self.channels])) + self.B*(1) + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSNNLayer_Htanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddSNNLayer_Htanh, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        self.first = AddSNNLayer_all()\n",
    "        self.second = AddSNNLayer_all(1)\n",
    "        self.sub = SubSNNLayer()\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,input1_val, input2_val, minimal_t_max=t_min+1)\n",
    "        tmin2, tmax2, second_val = self.second.set_params(t_min_prev, t_min,input1_val, input2_val, minimal_t_max=tmax1)\n",
    "\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,input1_val, input2_val, minimal_t_max=tmax2)\n",
    "\n",
    "        tmins, tmaxs, sub_val = self.sub.set_params(t_min, tmax1, first_val,second_val) ## t_min as angument do nothing\n",
    "        self.sub.t_max = tmins+1\n",
    "        return tmins, tmins+1, sub_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "        tj_first = self.first(tj1, tj2)\n",
    "        tj_second = self.second(tj1, tj2)\n",
    "        tj_sub = self.sub(tj_first, tj_second)\n",
    "        return tj_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingConv2D_all(nn.Module):\n",
    "    def __init__(self, filters, name, X_n=1, padding='same', kernel_size=(3,3), robustness_params=None, kernels = None, device = 'cuda:0', biases = None, stride=1):\n",
    "        super(SpikingConv2D_all, self).__init__()\n",
    "        if robustness_params is None:\n",
    "            robustness_params = {}\n",
    "        kernels_pos = torch.concat((kernels,-kernels), dim=1 )\n",
    "        if biases is not None:\n",
    "            biases_pos = biases\n",
    "        else:\n",
    "            biases_pos = None\n",
    "        kernels_new = kernels_pos\n",
    "        biases_new = biases_pos\n",
    "        self.conv_first = SpikingConv2D(filters, name, device = device, padding=padding, stride=stride, kernel_size=kernel_size,robustness_params=robustness_params, kernels=kernels_new, biases=biases_new)\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        tmin1, tmax1, first_val = self.conv_first.set_params(t_min_prev, t_min, in_ranges_max)\n",
    "        \n",
    "        # Returning for function signature consistency\n",
    "        return tmin1, tmax1, first_val\n",
    "        # return tmin1, self.t_max, torch.minimum(first_val, torch.ones(first_val.shape))\n",
    "\n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times tj, output spiking times ti. \n",
    "        \"\"\"\n",
    "        tj1 = self.conv_first(tj)\n",
    "        return tj1\n",
    "        # return tj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSNNBlock_all(nn.Module):\n",
    "    def __init__(self,resblock : ResidualBlock, in_channels, out_channels, stride=1, downsample=None, robustness_params = None, device = \"cuda:0\", end_maxpool = False):\n",
    "        super(ResidualSNNBlock_all, self).__init__()\n",
    "        conv = resblock.conv1[0]\n",
    "        bn= resblock.conv1[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        if (downsample is not None):\n",
    "            self.conv1 = SpikingConv2D_all(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        else:\n",
    "            self.conv1 = SpikingConv2D_Htanh(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        self.device = device\n",
    "        \n",
    "        conv = resblock.conv2[0]\n",
    "        bn= resblock.conv2[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        self.conv2 = SpikingConv2D_Htanh(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.identity = IdentitySNNLayer()\n",
    "        if end_maxpool:\n",
    "            self.add_layer = AddSNNLayer_all()\n",
    "        else:\n",
    "            self.add_layer = AddSNNLayer_Htanh()\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input_val, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        t_min1, t_max1, conv1_val = self.conv1.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "        self.t_max1 = t_max1\n",
    "        self.pooling1 = MaxMinPool2D(2, t_max1.data,2).to(self.device)\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, in_ranges_max=conv1_val)\n",
    "        max_out2 = t_max2 - t_min2\n",
    "        \n",
    "        if self.downsample:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "            self.t_max1_dummy = t_max1_dummy\n",
    "            self.pooling2 = MaxMinPool2D(2, t_max1_dummy.data,2).to(self.device)\n",
    "        else:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, minimal_t_max=t_max1_dummy, in_ranges_max=conv1_val)\n",
    "        \n",
    "        # time t_max2 and t_max1_dummy are the same\n",
    "        t_min_add = t_max2 - max(max_dummy1, max_out2)\n",
    "\n",
    "        self.t_min, self.t_max, add_val = self.add_layer.set_params(t_min_add, t_max2, conv2_val, downsample_val)\n",
    "\n",
    "        self.times = [(t_min1, t_max1, 'c'), (t_min2, t_max2, 'c'), (self.t_min, self.t_max, 'a') ]\n",
    "        return self.t_min, self.t_max, add_val\n",
    "\n",
    "    def get_main_times(self):\n",
    "        return self.times\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        out = self.conv1(x)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.pooling2(residual)\n",
    "            out = self.pooling1(out)\n",
    "            residual = torch.concat((residual,torch.ones(residual.shape)*self.t_max1_dummy), dim=1)\n",
    "            out = torch.concat((out,torch.ones(out.shape)*self.t_max1), dim=1)\n",
    "        else:\n",
    "            residual = self.identity(x)\n",
    "        print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        out = self.add_layer(out,residual) # no need for adding negative part\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSNN_all(nn.Module):\n",
    "    def __init__(self, layer, inplanes, planes, blocks, stride=1, device = 'cuda:0', end_maxpool = False):\n",
    "        self.inplanes = inplanes\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            conv2d, bias_from_nn = layer[0].downsample[0], layer[0].downsample[1]\n",
    "            conv_fused = fuse_conv_and_bn(conv2d, bias_from_nn)\n",
    "            robustness_params={\n",
    "                'noise':0.0,\n",
    "                'time_bits':0,\n",
    "                'weight_bits': 0,\n",
    "                'latency_quantiles':0.0\n",
    "            }\n",
    "            downsample = SpikingConv2D_all(planes,\"test2\", padding='same', stride=1, device=device,robustness_params=robustness_params,kernels=conv_fused.weight.data, biases=conv_fused.bias, kernel_size=(1,1))\n",
    "            # t_min, t_max = spiking_conv2.set_params(0,1)stride\n",
    "            \n",
    "        self.layers = []\n",
    "        self.layers.append(ResidualSNNBlock_all(layer[0],self.inplanes,planes, 1, downsample=downsample, device=device))\n",
    "        self.inplanes = planes\n",
    "        self.blocks = blocks\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                self.layers.append(ResidualSNNBlock_all(layer[i],self.inplanes,planes, 1, downsample=None, device=device, end_maxpool = True))\n",
    "            else:\n",
    "                self.layers.append(ResidualSNNBlock_all(layer[i],self.inplanes,planes, 1, downsample=None, device=device))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        tmin, tmax = t_min_prev, t_min\n",
    "        for i in range(self.blocks):\n",
    "            tmin, tmax, in_ranges_max = self.layers[i].set_params(tmin, tmax,in_ranges_max)\n",
    "        return tmin, tmax, in_ranges_max\n",
    "    \n",
    "    def get_main_times(self):\n",
    "        lst = []\n",
    "        for i in range(self.blocks):\n",
    "            lst.extend(self.layers[i].get_main_times())\n",
    "        return lst\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.blocks):\n",
    "            print(i)\n",
    "            x = self.layers[i].forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_params={\n",
    "    'noise':0.0,\n",
    "    'time_bits':0,\n",
    "    'weight_bits': 0,\n",
    "    'latency_quantiles':0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(4.5586, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "conv_fused = fuse_conv_and_bn(model2.conv1[0], model2.conv1[1])\n",
    "conv_first = SpikingConv2D(64, \"temp1\", device = 'cpu', padding=(3,3), stride=2, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "max_vect = torch.tensor([1,1,1,1,1])\n",
    "tmin, tmax, max_vect = conv_first.set_params(0,1,max_vect)\n",
    "print(tmin, tmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "model_conv1 = model2.conv1(random_input)\n",
    "print(model_conv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2543e-04, 0.0000e+00, 6.4826e-02, 2.8124e-02, 0.0000e+00, 1.0669e+00,\n",
      "        3.5316e-07, 0.0000e+00, 1.8713e-03, 0.0000e+00, 1.5711e-05, 5.7183e-05,\n",
      "        1.7728e-06, 1.9651e-05, 0.0000e+00, 1.4251e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.4909e-02, 0.0000e+00, 3.2300e-02, 1.6224e+00,\n",
      "        0.0000e+00, 3.6996e-02, 5.6862e-02, 1.2020e-06, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 5.0145e-06, 0.0000e+00, 8.9199e-03, 0.0000e+00,\n",
      "        1.5293e-02, 0.0000e+00, 4.6955e-02, 0.0000e+00, 5.3887e-03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2771e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.5586e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5823e-05, 3.8386e-03, 5.4359e-02,\n",
      "        0.0000e+00, 0.0000e+00, 5.7028e-03, 1.4734e+00],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(max_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tworzenie przykładowego array'a\n",
    "values = max_vect.detach().numpy()\n",
    "labels = [ i for i in range(len(max_vect))]\n",
    "\n",
    "# Tworzenie wykresu słupkowego\n",
    "plt.bar(labels, values, color='skyblue')\n",
    "\n",
    "# Dodanie etykiet\n",
    "plt.ylabel('$x_{max}$')\n",
    "plt.xlabel('Channel number')\n",
    "plt.title('maksymalne wartości każdego kanału')\n",
    "\n",
    "# Wyświetlenie wykresu\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 224, 224])\n",
      "tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "torch.Size([1, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "SNN_input = 1 - random_input\n",
    "\n",
    "# SNN_input = torch.concat((SNN_input, torch.ones(SNN_input.shape)),dim=1)\n",
    "print(SNN_input.shape)\n",
    "model2.conv1.eval() ## Important eval for batch normalization\n",
    "out1 = conv_first(SNN_input.to(\"cpu\"))\n",
    "# print(out1)\n",
    "out1_x = model2.conv1(random_input)\n",
    "temp = (conv_first.t_max - out1)\n",
    "# print((temp[0,:64] - temp[0,64:] - model_conv1).abs().max())\n",
    "print((temp - model_conv1).abs().max())\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cpu\")\n",
    "\n",
    "out2 = pool(out1)\n",
    "\n",
    "print(((tmax - out2) - model_maxpool).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsnn2 = AddSNNLayer_all(1)\n",
    "addsnn1 = AddSNNLayer_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "tensor(2.3842e-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "SNN_input1 = 1 - random_input +1\n",
    "SNN_input2 = 1 - random_input +1\n",
    "\n",
    "val_in1, val_in2 = torch.concat((torch.ones(5), torch.zeros(5))),torch.concat((torch.ones(5), torch.zeros(5)))\n",
    "tmin1, tmax1, val1 = addsnn1.set_params(0+1,1+1,val_in1,val_in2)\n",
    "tmin2, tmax2, val2 = addsnn2.set_params(0+1,1+1,val_in1,val_in2,tmax1)\n",
    "tmin1, tmax1, val1 = addsnn1.set_params(0+1,1+1,val_in1,val_in2,tmax2)\n",
    "\n",
    "outadd1 = addsnn1(torch.concat((SNN_input1, torch.ones(SNN_input1.shape)+1),dim=1),torch.concat((SNN_input2, torch.ones(SNN_input2.shape)+1),dim=1))\n",
    "print(outadd1.shape)\n",
    "print(((tmax1 - outadd1)[:5] - (tmax1 - outadd1)[5:] - F.relu(random_input*2)).abs().max())# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "tensor(2.3842e-07)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "outadd2 = addsnn2(torch.concat((SNN_input1, torch.ones(SNN_input1.shape)+1),dim=1),torch.concat((SNN_input2, torch.ones(SNN_input2.shape)+1),dim=1))\n",
    "print(outadd2.shape)\n",
    "print(((tmax1 - outadd2)[:5] - (tmax1 - outadd2)[5:] - F.relu(random_input*2-1)).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = SubSNNLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "tmins, tmaxs, sub_val = sub.set_params(0, tmax1, val1,val2)\n",
    "sub.t_max = tmins+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3842e-07)\n"
     ]
    }
   ],
   "source": [
    "outsub = sub(outadd1,outadd2)\n",
    "print(((sub.t_max-outsub)[:5] - (sub.t_max-outsub)[5:] - F.hardtanh(random_input*2)).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resblock test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "resblocksnn = ResidualSNNBlock_all(model2.layer0[0],64,64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.7246, grad_fn=<AddBackward0>) tensor(17.7246, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmin2, tmax2, max_vect2 = resblocksnn.set_params(tmin, tmax, torch.concat((max_vect, torch.zeros(max_vect.shape))))\n",
    "print(tmin2, tmax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "print(torch.concat((out2, torch.ones(out2.shape) * tmin),dim=1).shape)\n",
    "out3res = resblocksnn(torch.concat((out2, torch.ones(out2.shape) * tmax),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(3.2559e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((tmax2 - out3res)[:64].max())\n",
    "print(((tmax2 - out3res)[:64] - (tmax2 - out3res)[64:]  - model_resblock0).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5586, grad_fn=<AddBackward0>) tensor(17.7246, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(tmax, tmax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "tensor(26.0459, grad_fn=<AddBackward0>) tensor(27.0459, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "resblocksnn2 = ResidualSNNBlock_all(model2.layer0[1],64,64, device='cpu')\n",
    "tmin2, tmax2, max_vect2 = resblocksnn2.set_params(tmin2, tmax2, max_vect2)\n",
    "print(tmin2, tmax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "out4res = resblocksnn2(out3res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(5.4389e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((tmax2 - out4res)[:64].max())\n",
    "print(((tmax2 - out4res)[:64] - (tmax2 - out4res)[64:]  - model_resblock1).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "tensor(35.5240, grad_fn=<AddBackward0>) tensor(36.5240, grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 128, 56, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "resblocksnn3 = ResidualSNNBlock_all(model2.layer0[2],64,64, device='cpu')\n",
    "tmin2, tmax2, max_vect2 = resblocksnn3.set_params(tmin2, tmax2, max_vect2)\n",
    "print(tmin2, tmax2)\n",
    "out5res = resblocksnn3(out4res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(8.7619e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((tmax2 - out5res)[:64].max())\n",
    "print(((tmax2 - out5res)[:64] - (tmax2 - out5res)[64:]  - model_resblock2).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy = None\n",
    "\n",
    "# resblockSNN = ResidualSNNBlock(model2.layer0[0],64,64, downsample=dummy, device='cpu')\n",
    "# tmin, tmax, max_vect = resblockSNN.set_params(0,1, max_vect)\n",
    "# print(tmin,tmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.5240, grad_fn=<AddBackward0>) tensor(36.5240, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer0SNN = LayerSNN_all(model2.layer0, 64, 64, 3,device = 'cpu')\n",
    "tmax_prev = tmax\n",
    "tmin, tmax, max_vect = layer0SNN.set_params(tmin, tmax, torch.concat((max_vect, torch.zeros(max_vect.shape))))\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 128, 56, 56])\n",
      "1\n",
      "torch.Size([1, 128, 56, 56])\n",
      "2\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n",
      "tensor(8.7619e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_3 = layer0SNN.forward(torch.concat((out2, torch.ones(out2.shape) * tmax_prev),dim=1))\n",
    "print(out_3.shape)\n",
    "print(((tmax - out_3)[:64] - (tmax - out_3)[64:] - model_layer0).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "tensor(105.7493, grad_fn=<AddBackward0>) tensor(106.7493, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "layer1SNN = LayerSNN_all(model2.layer1, 64, 128, 4,device = 'cpu')\n",
    "tmin, tmax, max_vect = layer1SNN.set_params(tmin, tmax, max_vect)\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 256, 28, 28])\n",
      "1\n",
      "torch.Size([1, 256, 28, 28])\n",
      "2\n",
      "torch.Size([1, 256, 28, 28])\n",
      "3\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([256, 28, 28])\n",
      "tensor(3.8342e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_4 = layer1SNN.forward(out_3)\n",
    "print((tmax - out_4).shape)\n",
    "print(((tmax - out_4)[ :128] - (tmax - out_4)[ 128:] - model_layer1).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(260.7690, grad_fn=<AddBackward0>) tensor(261.7690, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer2SNN = LayerSNN_all(model2.layer2, 128, 256, 6,device = 'cpu')\n",
    "tmin, tmax, max_vect = layer2SNN.set_params(tmin, tmax, max_vect)\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 512, 14, 14])\n",
      "1\n",
      "torch.Size([1, 512, 14, 14])\n",
      "2\n",
      "torch.Size([1, 512, 14, 14])\n",
      "3\n",
      "torch.Size([1, 512, 14, 14])\n",
      "4\n",
      "torch.Size([1, 512, 14, 14])\n",
      "5\n",
      "torch.Size([1, 512, 14, 14])\n",
      "tensor(0.0001, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_5 = layer2SNN.forward(out_4)\n",
    "\n",
    "print(((tmax - out_5)[:256] - (tmax - out_5)[256:] - model_layer2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\1686484564.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3157384604.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\3950266273.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\623020791.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(467.1662, grad_fn=<AddBackward0>) tensor(477.2852, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer3SNN = LayerSNN_all(model2.layer3, 256, 512, 3,device = 'cpu',end_maxpool=True)\n",
    "tmin, tmax, max_vect = layer3SNN.set_params(tmin, tmax, max_vect)\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "1\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "2\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "tensor(0.0001, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_6 = layer3SNN.forward(out_5)\n",
    "\n",
    "print(((tmax - out_6)[:512] - model_layer3).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cpu\")\n",
    "\n",
    "out7 = pool2(out_6[:512])\n",
    "\n",
    "print(((tmax - out7) - model_maxpool2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(477.2852, grad_fn=<AddBackward0>) tensor(501.3414, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\2453877980.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\2453877980.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\2453877980.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_9892\\2453877980.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "spiking_dense = SpikingDense(10,\"test\",robustness_params=robustness_params)\n",
    "weights = model2.fc.weight.T\n",
    "biases = model2.fc.bias\n",
    "spiking_dense.build((512,),weights, biases)\n",
    "tmin, tmax, max_vect = spiking_dense.set_params(tmin, tmax, max_vect[:512])\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor(7.3433e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out8 = spiking_dense(out7.view(out7.size(0), -1))\n",
    "\n",
    "print((tmax - out8 - model_linear).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_lst = [layer0SNN, layer1SNN, layer2SNN, layer3SNN]\n",
    "ll = []\n",
    "for i in layer_lst:\n",
    "    ll.extend(i.get_main_times())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(8.7444, grad_fn=<AddBackward0>), tensor(9.7444, grad_fn=<AddBackward0>), 'c'), (tensor(11.0958, grad_fn=<AddBackward0>), tensor(12.0958, grad_fn=<AddBackward0>), 'c'), (tensor(16.7246, grad_fn=<AddBackward0>), tensor(17.7246, grad_fn=<AddBackward0>), 'a'), (tensor(17.7247, grad_fn=<AddBackward0>), tensor(18.7247, grad_fn=<AddBackward0>), 'c'), (tensor(20.3534, grad_fn=<SubBackward0>), tensor(21.3534, grad_fn=<AddBackward0>), 'c'), (tensor(26.0459, grad_fn=<AddBackward0>), tensor(27.0459, grad_fn=<AddBackward0>), 'a'), (tensor(27.3232, grad_fn=<AddBackward0>), tensor(28.3232, grad_fn=<AddBackward0>), 'c'), (tensor(29.7384, grad_fn=<SubBackward0>), tensor(30.7384, grad_fn=<AddBackward0>), 'c'), (tensor(35.5240, grad_fn=<AddBackward0>), tensor(36.5240, grad_fn=<AddBackward0>), 'a'), (tensor(36.5240, grad_fn=<AddBackward0>), tensor(41.0255, grad_fn=<AddBackward0>), 'c'), (tensor(55.4538, grad_fn=<AddBackward0>), tensor(56.4538, grad_fn=<AddBackward0>), 'c'), (tensor(63.4721, grad_fn=<AddBackward0>), tensor(64.4721, grad_fn=<AddBackward0>), 'a'), (tensor(64.4722, grad_fn=<AddBackward0>), tensor(65.4722, grad_fn=<AddBackward0>), 'c'), (tensor(69.4903, grad_fn=<SubBackward0>), tensor(70.4903, grad_fn=<AddBackward0>), 'c'), (tensor(77.5272, grad_fn=<AddBackward0>), tensor(78.5272, grad_fn=<AddBackward0>), 'a'), (tensor(78.5273, grad_fn=<AddBackward0>), tensor(79.5273, grad_fn=<AddBackward0>), 'c'), (tensor(83.5641, grad_fn=<SubBackward0>), tensor(84.5641, grad_fn=<AddBackward0>), 'c'), (tensor(91.6196, grad_fn=<AddBackward0>), tensor(92.6196, grad_fn=<AddBackward0>), 'a'), (tensor(92.6198, grad_fn=<AddBackward0>), tensor(93.6198, grad_fn=<AddBackward0>), 'c'), (tensor(97.6751, grad_fn=<SubBackward0>), tensor(98.6751, grad_fn=<AddBackward0>), 'c'), (tensor(105.7493, grad_fn=<AddBackward0>), tensor(106.7493, grad_fn=<AddBackward0>), 'a'), (tensor(106.7493, grad_fn=<AddBackward0>), tensor(120.5260, grad_fn=<AddBackward0>), 'c'), (tensor(146.9743, grad_fn=<AddBackward0>), tensor(147.9743, grad_fn=<AddBackward0>), 'c'), (tensor(157.9900, grad_fn=<AddBackward0>), tensor(158.9900, grad_fn=<AddBackward0>), 'a'), (tensor(159.6799, grad_fn=<AddBackward0>), tensor(160.6799, grad_fn=<AddBackward0>), 'c'), (tensor(167.0057, grad_fn=<SubBackward0>), tensor(168.0057, grad_fn=<AddBackward0>), 'c'), (tensor(178.0770, grad_fn=<AddBackward0>), tensor(179.0770, grad_fn=<AddBackward0>), 'a'), (tensor(179.8347, grad_fn=<AddBackward0>), tensor(180.8347, grad_fn=<AddBackward0>), 'c'), (tensor(187.1483, grad_fn=<SubBackward0>), tensor(188.1483, grad_fn=<AddBackward0>), 'c'), (tensor(198.2354, grad_fn=<AddBackward0>), tensor(199.2354, grad_fn=<AddBackward0>), 'a'), (tensor(200.0919, grad_fn=<AddBackward0>), tensor(201.0919, grad_fn=<AddBackward0>), 'c'), (tensor(207.3224, grad_fn=<SubBackward0>), tensor(208.3224, grad_fn=<AddBackward0>), 'c'), (tensor(218.7710, grad_fn=<AddBackward0>), tensor(219.7710, grad_fn=<AddBackward0>), 'a'), (tensor(220.5589, grad_fn=<AddBackward0>), tensor(221.5589, grad_fn=<AddBackward0>), 'c'), (tensor(228.2195, grad_fn=<SubBackward0>), tensor(229.2195, grad_fn=<AddBackward0>), 'c'), (tensor(239.6980, grad_fn=<AddBackward0>), tensor(240.6980, grad_fn=<AddBackward0>), 'a'), (tensor(241.6788, grad_fn=<AddBackward0>), tensor(242.6788, grad_fn=<AddBackward0>), 'c'), (tensor(249.1765, grad_fn=<SubBackward0>), tensor(250.1765, grad_fn=<AddBackward0>), 'c'), (tensor(260.7690, grad_fn=<AddBackward0>), tensor(261.7690, grad_fn=<AddBackward0>), 'a'), (tensor(261.7690, grad_fn=<AddBackward0>), tensor(288.7417, grad_fn=<AddBackward0>), 'c'), (tensor(396.8598, grad_fn=<AddBackward0>), tensor(397.8598, grad_fn=<AddBackward0>), 'c'), (tensor(407.5773, grad_fn=<AddBackward0>), tensor(408.5773, grad_fn=<AddBackward0>), 'a'), (tensor(409.9489, grad_fn=<AddBackward0>), tensor(410.9489, grad_fn=<AddBackward0>), 'c'), (tensor(416.2948, grad_fn=<SubBackward0>), tensor(417.2948, grad_fn=<AddBackward0>), 'c'), (tensor(427.4138, grad_fn=<AddBackward0>), tensor(428.4138, grad_fn=<AddBackward0>), 'a'), (tensor(455.3458, grad_fn=<AddBackward0>), tensor(456.3458, grad_fn=<AddBackward0>), 'c'), (tensor(466.1662, grad_fn=<AddBackward0>), tensor(467.1662, grad_fn=<AddBackward0>), 'c'), (tensor(467.1662, grad_fn=<AddBackward0>), tensor(477.2852, grad_fn=<AddBackward0>), 'a')]\n"
     ]
    }
   ],
   "source": [
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [i[1].detach().numpy() for i in ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26c23c5eb50>]"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tt)\n",
    "# plt.ylim([0,2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2 = [(tt[i])/(tt[i-1]) for i in range(2,len(tt))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26c2d74dca0>]"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tt2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
