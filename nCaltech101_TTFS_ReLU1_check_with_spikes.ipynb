{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a85a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4b715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same',bias=False),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same',bias=False),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1,bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=0.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_chanels = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_chanels, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same',bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x, min_val=0.0, max_val=1.0)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38607beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=0.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_channels = 5):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x,min_val=0, max_val=1)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x# F.hardtanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98164782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_resnet = ResNet(ResidualBlock, [5, 6, 6, 4], in_channels = 6, num_classes=101).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c8f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = ResNet(ResidualBlock, [5, 6, 6, 4], num_classes = 101, in_channels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc3c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer0): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=101, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet.load_state_dict(torch.load(\"best_resnet_nCaltech101_ReLU1_ReLUmaxpool_EST__FC2_corrected_exp_copy1.pt\", weights_only=True))\n",
    "model_resnet.eval().to(\"cuda\")\n",
    "print(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e2765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2163,  0.4308, -1.1207, -3.5953,  1.6711,  1.5722,  1.7449,  0.7376,\n",
       "         -3.8387,  0.9331,  1.3603, -4.8497, -2.3277,  0.3880, -3.4415, -5.0047,\n",
       "         -0.3977, -6.0568,  1.5379, -2.9769, -0.9855,  0.7568,  2.3783,  1.0269,\n",
       "         -2.4769, -0.4099,  1.4334, -2.0948, -3.3150,  1.1027, -0.8569,  0.0149,\n",
       "         -1.6380, -3.6980, -0.1839,  2.6007,  1.1846,  0.1530, -1.5050, -1.8727,\n",
       "          0.6083, -2.7449, -0.2699, -2.9457, -4.5308,  1.4536, -2.6619,  5.2084,\n",
       "         -5.5468, -2.3732, -5.0009, -1.0081,  1.5027, -1.6777, -3.3897,  0.4584,\n",
       "          1.3667, -1.3254, -1.9281, -0.7660, -1.4549, -1.1950,  1.8507, -0.5560,\n",
       "          0.9874, -2.6832,  0.6909, -2.3641, -3.1094,  0.6242, -1.7520, -3.2381,\n",
       "          2.4555, -2.7448,  0.7174, -1.1286, -2.0757, -2.7847,  1.0638, -2.4839,\n",
       "         -2.5655, -2.7873,  2.5571, -6.8863, -2.7477, -1.5022,  1.7753, -2.4863,\n",
       "          0.7694,  3.8319, -1.4426, -2.2564,  6.5023, -3.5000,  0.1364, -2.3715,\n",
       "         -4.0597,  3.9817,  0.3742, -1.3501, -3.2962]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a879ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SNN.Check_ReLU1 import fuse_conv_and_bn\n",
    "from SNN.Check_ReLU1 import SpikingConv2D\n",
    "from SNN.Check_ReLU1 import MaxMinPool2D\n",
    "from SNN.Check_ReLU1 import LayerSNN_ReLU1\n",
    "from SNN.Check_ReLU1 import SpikingDense_positive_ReLU1, SpikingDense_pos_all, SpikingDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74e5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_ttfs(nn.Module):\n",
    "    def __init__(self, model : ResNet, in_channels = 5):\n",
    "        super(ResNet_ttfs, self).__init__()\n",
    "        model.eval()\n",
    "        robustness_params={\n",
    "            'noise':0.0,\n",
    "            'time_bits':0,\n",
    "            'weight_bits': 0,\n",
    "            'latency_quantiles':0.0\n",
    "        }\n",
    "        model.to('cuda')\n",
    "        conv_fused = fuse_conv_and_bn(model.conv1[0], model.conv1[1], device = 'cuda')\n",
    "        self.conv_first = SpikingConv2D(64, \"temp1\", device = 'cuda', padding=(3,3), stride=2, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        max_vect = torch.tensor([1]*in_channels)\n",
    "        tmin, tmax, max_vect, scalar = self.conv_first.set_params(0.0,1.0,max_vect)\n",
    "        self.tmin_post_pool = tmax\n",
    "        self.pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cuda\")\n",
    "        self.layer0SNN = LayerSNN_ReLU1(model.layer0, 64, 64, 5,device = 'cuda')\n",
    "        tmax_prev = tmax\n",
    "        tmin, tmax, max_vect, scalar = self.layer0SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer1SNN = LayerSNN_ReLU1(model.layer1, 64, 128, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer1SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer2SNN = LayerSNN_ReLU1(model.layer2, 128, 256, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer2SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer3SNN = LayerSNN_ReLU1(model.layer3, 256, 512, 4,device = 'cuda',end_maxpool=True)\n",
    "        tmin, tmax, max_vect, scalar = self.layer3SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cuda\")\n",
    "        # temp = torch.ones((512,2,2))\n",
    "        # max_vect = ((temp.T)*max_vect[:512]).T\n",
    "        # max_vect = max_vect.view(-1)\n",
    "\n",
    "        max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect).T.contiguous().view(512).to(\"cuda\")\n",
    "\n",
    "        weights = model.fc.weight.detach().clone()\n",
    "        biases = model.fc.bias.detach().clone()\n",
    "        self.layer_fc = SpikingDense_positive_ReLU1(256, 512,\"test\",robustness_params=robustness_params,device = 'cuda',weights=weights, biases=biases)\n",
    "        \n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer_fc2 = SpikingDense_pos_all(101,256, '',model.fc2.weight, model.fc2.bias,robustness_params=robustness_params,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc2.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.tmin, self.tmax = tmin, tmax\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        spike_sum, v_sqare_sum = 0,0.0\n",
    "\n",
    "        x, spike, v  = self.conv_first(tj)\n",
    "\n",
    "        spike_sum += spike\n",
    "        v_sqare_sum +=v\n",
    "        x, spike = self.pool(x)\n",
    "\n",
    "        spike_sum += spike\n",
    "        # print(x.shape)\n",
    "        x = x\n",
    "        x, spike1, v1 = self.layer0SNN(x)\n",
    "\n",
    "        x, spike2, v2 = self.layer1SNN(x)\n",
    "\n",
    "        x, spike3, v3 = self.layer2SNN(x)\n",
    "\n",
    "        x, spike4, v4 = self.layer3SNN(x)\n",
    "\n",
    "        # print(x[1].max(),x[1].min())\n",
    "        x, spike5 = self.pool2(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x, spike6, v5 = self.layer_fc(x)\n",
    "\n",
    "        x, spike7, v6 = self.layer_fc2(x)\n",
    "\n",
    "        spike_sum += spike1+spike2+spike3+spike4+spike5+spike6+spike7\n",
    "        v_sqare_sum += v1+v2+v3+v4+v5+v6\n",
    "        return x, spike_sum, v_sqare_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a9880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V+eps_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:311: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:313: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:565: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:566: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:582: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:634: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:635: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:650: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:652: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:654: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:751: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:752: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:771: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:775: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:773: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_20752\\1005372051.py:32: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect).T.contiguous().view(512).to(\"cuda\")\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:199: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:200: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "model_ttfs = ResNet_ttfs(model_resnet,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ab7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6456915, device='cuda:0') tensor(529491.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([ 2.8947,  0.4039, -1.2676, -3.5619,  1.7669,  1.6346,  1.9091,  0.6477,\n",
      "        -3.7858,  1.2738,  1.4795, -4.9124, -2.0185,  0.3367, -3.1000, -4.8390,\n",
      "        -0.5208, -6.1394,  1.3058, -2.6729, -0.7281,  0.7290,  2.5128,  1.0366,\n",
      "        -2.2848, -0.3446,  1.4807, -2.1247, -3.3326,  1.3377, -1.1378, -0.0867,\n",
      "        -1.5823, -3.7456, -0.1256,  2.8010,  1.3228,  0.2430, -1.4812, -1.9609,\n",
      "         0.2256, -2.5879, -0.2733, -2.8508, -4.3779,  1.3920, -2.8964,  5.1185,\n",
      "        -5.5012, -2.6086, -4.6218, -0.8687,  1.5102, -1.6151, -3.3670,  0.6029,\n",
      "         1.3008, -1.2377, -1.7246, -0.7928, -1.5728, -1.3493,  1.9646, -0.5245,\n",
      "         0.5710, -2.3229,  0.7286, -2.2595, -3.1004,  0.4798, -1.8245, -3.3902,\n",
      "         2.5551, -3.0262,  0.5963, -1.1345, -2.2159, -2.8213,  0.9346, -2.4465,\n",
      "        -2.7852, -2.7342,  2.3237, -6.7722, -2.9644, -1.7615,  1.5442, -2.1599,\n",
      "         0.7501,  3.8596, -1.5002, -2.0214,  6.1713, -3.7779,  0.2836, -2.4946,\n",
      "        -3.6884,  4.0951,  0.4445, -1.6595, -3.5739], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "temp_ttfs = 1 - temp\n",
    "# print(model_ttfs.forward(temp_ttfs))\n",
    "out, spike_sum, v_sqare_sum = model_ttfs.forward(temp_ttfs.to(\"cuda\"))\n",
    "\n",
    "print(spike_sum, v_sqare_sum)\n",
    "print(((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a5e359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.9002,  0.4078, -1.2630, -3.5515,  1.7679,  1.6281,  1.9090,  0.6418,\n",
       "         -3.7841,  1.2735,  1.4803, -4.9193, -2.0207,  0.3394, -3.1032, -4.8344,\n",
       "         -0.5181, -6.1425,  1.3027, -2.6710, -0.7210,  0.7268,  2.5181,  1.0448,\n",
       "         -2.2818, -0.3474,  1.4786, -2.1328, -3.3226,  1.3367, -1.1439, -0.0896,\n",
       "         -1.5788, -3.7460, -0.1333,  2.8032,  1.3185,  0.2451, -1.4808, -1.9630,\n",
       "          0.2196, -2.5879, -0.2691, -2.8526, -4.3784,  1.3905, -2.8941,  5.1162,\n",
       "         -5.4992, -2.6100, -4.6183, -0.8585,  1.5119, -1.6161, -3.3759,  0.5940,\n",
       "          1.2981, -1.2374, -1.7209, -0.7953, -1.5695, -1.3563,  1.9699, -0.5235,\n",
       "          0.5704, -2.3181,  0.7195, -2.2651, -3.1022,  0.4874, -1.8184, -3.3926,\n",
       "          2.5557, -3.0322,  0.5854, -1.1347, -2.2217, -2.8287,  0.9368, -2.4488,\n",
       "         -2.7861, -2.7304,  2.3245, -6.7750, -2.9689, -1.7672,  1.5368, -2.1581,\n",
       "          0.7554,  3.8485, -1.4995, -2.0207,  6.1667, -3.7898,  0.2828, -2.4903,\n",
       "         -3.6815,  4.1028,  0.4511, -1.6578, -3.5783]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea43df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "torch.manual_seed(19)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    # v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(60),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])\n",
    "\n",
    "\n",
    "class NCaltech101ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir_file, transform=None, target_transform=None):\n",
    "        self.images = np.load(img_dir_file + '_x.npy')\n",
    "        self.labels = np.load(img_dir_file + '_y.npy')\n",
    "        self.transform = transform\n",
    "        self.transform = transform\n",
    "        self.stage = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        label_temp = np.zeros((101,))\n",
    "        label_temp[label] = 1\n",
    "        if self.stage == 0:\n",
    "            return self.transform(torch.tensor(image)), torch.tensor(label_temp)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label_temp)\n",
    "    \n",
    "    def set_stage(self, stage):\n",
    "        self.stage = stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b195df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# training_data = NCarsImageDataset(\"./Datasety/nCars_train_EST_exp_\", transform=transforms)\n",
    "data = NCaltech101ImageDataset(\"./Datasety/Ncaltech101_EST_exp_corr\", transform=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b566e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8709/8709 [1:11:43<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6672495.5000, device='cuda:0')\n",
      "Mean MAE: tensor(0.0088, device='cuda:0')\n",
      "Mean MSE: tensor(0.0011, device='cuda:0')\n",
      "accuracy: 0.7449764611321621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.set_stage(1)\n",
    "spike_acc = 0\n",
    "MAE_sum = 0\n",
    "MSE_sum = 0\n",
    "TP = 0\n",
    "for i in tqdm(range(len(data))):#10\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            temp_data, temp_label = deepcopy(data[i])\n",
    "            temp_ttfs = (1 - temp_data).unsqueeze(0).to(\"cuda\")\n",
    "            temp_data = temp_data.to(\"cuda\")\n",
    "            # print(temp_data.shape)\n",
    "            # print(model_ttfs.forward(temp_ttfs))\n",
    "            out, spike_sum, v_sqare_sum = model_ttfs.forward(temp_ttfs)\n",
    "            spike_acc+=spike_sum\n",
    "            output = ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:]\n",
    "            output_true = model_resnet(temp_data.unsqueeze(0))\n",
    "            MAE = torch.sum((output-output_true).abs())/101.0\n",
    "            MSE = torch.sqrt(torch.sum((output-output_true)*(output-output_true)))\n",
    "            MAE_sum += MAE\n",
    "            MSE_sum += MSE\n",
    "            TP += (torch.argmax(temp_label)==torch.argmax(output)) \n",
    "            del temp_data, temp_ttfs\n",
    "    if i%100==0:\n",
    "        torch.cuda.empty_cache()\n",
    "print(spike_acc/len(data))\n",
    "print(\"Mean MAE:\", MAE_sum/len(data))\n",
    "print(\"Mean MSE:\", MSE_sum/(len(data)*101.0))\n",
    "print(\"accuracy:\",float(TP)/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5066f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6488, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f8addfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16) tensor(16, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = 2201\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        temp_data, temp_label = deepcopy(data[i])\n",
    "        temp_ttfs = (1 - temp_data).unsqueeze(0).to(\"cuda\")\n",
    "        temp_data = temp_data.to(\"cuda\")\n",
    "        # print(temp_data.shape)\n",
    "        # print(model_ttfs.forward(temp_ttfs))\n",
    "        out, spike_sum, v_sqare_sum = model_ttfs.forward(temp_ttfs)\n",
    "        spike_acc+=spike_sum\n",
    "        output = ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:]\n",
    "        output_true = model_resnet(temp_data.unsqueeze(0))\n",
    "        MAE = torch.sum((output-output_true).abs())/101.0\n",
    "        MSE = torch.sqrt(torch.sum((output-output_true)*(output-output_true)))\n",
    "        MAE_sum += MAE\n",
    "        MSE_sum += MSE\n",
    "        TP += (torch.argmax(temp_label)==torch.argmax(output)) \n",
    "        print(torch.argmax(temp_label), torch.argmax(output))\n",
    "        del temp_data, temp_ttfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
