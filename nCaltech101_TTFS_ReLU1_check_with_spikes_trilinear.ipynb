{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a85a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4b715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same',bias=False),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same',bias=False),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1,bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=0.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_chanels = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_chanels, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same',bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x, min_val=0.0, max_val=1.0)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38607beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=0.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=0.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_channels = 5):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x,min_val=0, max_val=1)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x# F.hardtanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98164782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_resnet = ResNet(ResidualBlock, [5, 6, 6, 4], in_channels = 6, num_classes=101).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c8f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = ResNet(ResidualBlock, [5, 6, 6, 4], num_classes = 101, in_channels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc3c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer0): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=0.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=101, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet = torch.load(\"best_resnet_nCaltech101_ReLU1_ReLUmaxpool_EST_FC2_corrected_exp.pt\", weights_only=False)\n",
    "model_resnet.eval().to(\"cuda\")\n",
    "print(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e2765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0439, -2.5614,  1.0997, -3.3898,  0.7834,  1.5576, -0.8800,  0.6590,\n",
       "         -1.5126,  1.0864,  0.1628, -1.8188,  0.2477, -1.4779, -0.5599, -1.8114,\n",
       "         -0.3942, -0.7469,  0.4051, -0.7018, -3.4831, -1.8707, -2.9679, -1.3461,\n",
       "         -3.1930, -2.1993, -0.6713,  1.3067, -5.1448, -1.8813,  1.5626,  0.7617,\n",
       "          0.1341, -2.5766, -1.0187, -0.8682,  1.2351, -1.6846, -0.3800,  1.2141,\n",
       "          1.6978, -1.5449, -1.5274,  2.7839, -1.1501, -1.1818, -1.6145,  0.2546,\n",
       "         -0.9642,  0.5107,  1.0889, -2.7781, -0.5333,  0.8462,  1.5289,  2.1188,\n",
       "         -1.2613, -1.6228, -2.2690,  0.5117,  0.2113,  0.8845,  0.3006,  0.7372,\n",
       "          1.4158, -4.1189,  1.6858, -0.9459, -4.8902, -3.1649, -1.1440,  2.5721,\n",
       "         -0.6321, -2.0204,  3.3315, -2.0412, -3.2814, -0.8243, -2.3256, -0.1700,\n",
       "         -1.9981, -1.4495, -0.2801, -2.6389,  0.5912,  1.9115,  1.3035, -1.6153,\n",
       "         -2.6138,  2.3971, -0.9295, -2.7658,  0.6637, -0.7801, -1.2309, -0.3919,\n",
       "         -3.4303, -2.1354, -2.1204, -1.0545, -0.3945]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a879ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SNN.Check_ReLU1 import fuse_conv_and_bn\n",
    "from SNN.Check_ReLU1 import SpikingConv2D\n",
    "from SNN.Check_ReLU1 import MaxMinPool2D\n",
    "from SNN.Check_ReLU1 import LayerSNN_ReLU1\n",
    "from SNN.Check_ReLU1 import SpikingDense_positive_ReLU1, SpikingDense_pos_all, SpikingDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74e5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_ttfs(nn.Module):\n",
    "    def __init__(self, model : ResNet, in_channels = 5):\n",
    "        super(ResNet_ttfs, self).__init__()\n",
    "        model.eval()\n",
    "        robustness_params={\n",
    "            'noise':0.0,\n",
    "            'time_bits':0,\n",
    "            'weight_bits': 0,\n",
    "            'latency_quantiles':0.0\n",
    "        }\n",
    "        model.to('cuda')\n",
    "        conv_fused = fuse_conv_and_bn(model.conv1[0], model.conv1[1], device = 'cuda')\n",
    "        self.conv_first = SpikingConv2D(64, \"temp1\", device = 'cuda', padding=(3,3), stride=2, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        max_vect = torch.tensor([1]*in_channels)\n",
    "        tmin, tmax, max_vect, scalar = self.conv_first.set_params(0.0,1.0,max_vect)\n",
    "        self.tmin_post_pool = tmax\n",
    "        self.pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cuda\")\n",
    "        self.layer0SNN = LayerSNN_ReLU1(model.layer0, 64, 64, 5,device = 'cuda')\n",
    "        tmax_prev = tmax\n",
    "        tmin, tmax, max_vect, scalar = self.layer0SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer1SNN = LayerSNN_ReLU1(model.layer1, 64, 128, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer1SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer2SNN = LayerSNN_ReLU1(model.layer2, 128, 256, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer2SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer3SNN = LayerSNN_ReLU1(model.layer3, 256, 512, 4,device = 'cuda',end_maxpool=True)\n",
    "        tmin, tmax, max_vect, scalar = self.layer3SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cuda\")\n",
    "        # temp = torch.ones((512,2,2))\n",
    "        # max_vect = ((temp.T)*max_vect[:512]).T\n",
    "        # max_vect = max_vect.view(-1)\n",
    "\n",
    "        max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect).T.contiguous().view(512).to(\"cuda\")\n",
    "\n",
    "        weights = model.fc.weight.detach().clone()\n",
    "        biases = model.fc.bias.detach().clone()\n",
    "        self.layer_fc = SpikingDense_positive_ReLU1(256, 512,\"test\",robustness_params=robustness_params,device = 'cuda',weights=weights, biases=biases)\n",
    "        \n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer_fc2 = SpikingDense_pos_all(101,256, '',model.fc2.weight, model.fc2.bias,robustness_params=robustness_params,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc2.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.tmin, self.tmax = tmin, tmax\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        spike_sum, v_sqare_sum = 0,0.0\n",
    "\n",
    "        x, spike, v  = self.conv_first(tj)\n",
    "\n",
    "        spike_sum += spike\n",
    "        v_sqare_sum +=v\n",
    "        x, spike = self.pool(x)\n",
    "\n",
    "        spike_sum += spike\n",
    "        # print(x.shape)\n",
    "        x = x\n",
    "        x, spike1, v1 = self.layer0SNN(x)\n",
    "\n",
    "        x, spike2, v2 = self.layer1SNN(x)\n",
    "\n",
    "        x, spike3, v3 = self.layer2SNN(x)\n",
    "\n",
    "        x, spike4, v4 = self.layer3SNN(x)\n",
    "\n",
    "        # print(x[1].max(),x[1].min())\n",
    "        x, spike5 = self.pool2(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x, spike6, v5 = self.layer_fc(x)\n",
    "\n",
    "        x, spike7, v6 = self.layer_fc2(x)\n",
    "\n",
    "        spike_sum += spike1+spike2+spike3+spike4+spike5+spike6+spike7\n",
    "        v_sqare_sum += v1+v2+v3+v4+v5+v6\n",
    "        return x, spike_sum, v_sqare_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a9880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V+eps_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:311: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:313: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:565: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:566: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:582: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:634: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:635: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:650: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:652: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:654: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:751: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:752: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:771: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:775: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:773: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_5400\\1005372051.py:32: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect).T.contiguous().view(512).to(\"cuda\")\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:199: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_ReLU1.py:200: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "model_ttfs = ResNet_ttfs(model_resnet,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ab7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10489619, device='cuda:0') tensor(780884.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([-2.3047, -2.6129,  1.0238, -3.5191,  0.8542,  1.5927, -0.8449,  0.6999,\n",
      "        -1.6516,  1.3530,  0.3256, -1.8877,  0.3147, -1.5017, -0.3977, -1.6101,\n",
      "        -0.4186, -0.6821,  0.4235, -0.8002, -3.6488, -2.0146, -3.1218, -1.3300,\n",
      "        -3.2068, -2.3535, -0.6922,  1.4187, -5.3874, -1.8285,  1.5525,  0.4871,\n",
      "         0.1841, -2.7197, -1.1802, -0.8284,  1.2499, -1.8337, -0.2663,  1.1777,\n",
      "         1.5968, -1.5436, -1.7056,  3.0666, -1.1822, -1.2031, -1.8047,  0.2087,\n",
      "        -0.8320,  0.5121,  1.4110, -2.7447, -0.6277,  0.8506,  1.4823,  2.1850,\n",
      "        -1.3260, -1.7684, -2.2366,  0.7277,  0.3308,  0.8747,  0.2011,  0.5919,\n",
      "         1.4320, -3.9982,  1.8748, -1.0424, -5.0349, -3.3587, -1.1495,  2.7507,\n",
      "        -0.4605, -2.1810,  3.4284, -2.1270, -3.4800, -0.8655, -2.5851, -0.1547,\n",
      "        -2.3788, -1.4022, -0.3896, -2.6428,  0.7120,  1.9771,  1.2329, -1.5045,\n",
      "        -2.6971,  2.4494, -0.9440, -2.9599,  0.4601, -0.9436, -1.0750, -0.2619,\n",
      "        -3.4933, -2.1270, -2.3551, -1.2217, -0.4352], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "temp_ttfs = 1 - temp\n",
    "# print(model_ttfs.forward(temp_ttfs))\n",
    "out, spike_sum, v_sqare_sum = model_ttfs.forward(temp_ttfs.to(\"cuda\"))\n",
    "\n",
    "print(spike_sum, v_sqare_sum)\n",
    "print(((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a5e359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3083, -2.6151,  1.0211, -3.5246,  0.8565,  1.5948, -0.8491,  0.6985,\n",
       "         -1.6531,  1.3559,  0.3292, -1.8883,  0.3204, -1.5038, -0.3959, -1.6078,\n",
       "         -0.4187, -0.6859,  0.4219, -0.7995, -3.6475, -2.0176, -3.1252, -1.3330,\n",
       "         -3.2106, -2.3568, -0.6938,  1.4213, -5.3913, -1.8258,  1.5492,  0.4836,\n",
       "          0.1815, -2.7211, -1.1813, -0.8289,  1.2542, -1.8368, -0.2622,  1.1782,\n",
       "          1.5958, -1.5410, -1.7070,  3.0704, -1.1848, -1.2054, -1.8063,  0.2080,\n",
       "         -0.8305,  0.5123,  1.4179, -2.7414, -0.6261,  0.8560,  1.4841,  2.1888,\n",
       "         -1.3257, -1.7683, -2.2408,  0.7288,  0.3290,  0.8740,  0.2032,  0.5950,\n",
       "          1.4307, -3.9975,  1.8764, -1.0434, -5.0378, -3.3639, -1.1498,  2.7539,\n",
       "         -0.4584, -2.1867,  3.4322, -2.1307, -3.4832, -0.8650, -2.5867, -0.1495,\n",
       "         -2.3814, -1.4034, -0.3902, -2.6420,  0.7140,  1.9767,  1.2312, -1.5042,\n",
       "         -2.6960,  2.4518, -0.9463, -2.9616,  0.4576, -0.9474, -1.0724, -0.2636,\n",
       "         -3.4938, -2.1277, -2.3551, -1.2260, -0.4347]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea43df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "torch.manual_seed(19)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    # v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(60),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])\n",
    "\n",
    "\n",
    "class NCaltech101ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir_file, transform=None, target_transform=None):\n",
    "        self.images = np.load(img_dir_file + '_x.npy')\n",
    "        self.labels = np.load(img_dir_file + '_y.npy')\n",
    "        self.transform = transform\n",
    "        self.transform = transform\n",
    "        self.stage = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        label_temp = np.zeros((101,))\n",
    "        label_temp[label] = 1\n",
    "        if self.stage == 0:\n",
    "            return self.transform(torch.tensor(image)), torch.tensor(label_temp)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label_temp)\n",
    "    \n",
    "    def set_stage(self, stage):\n",
    "        self.stage = stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b195df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# training_data = NCarsImageDataset(\"./Datasety/nCars_train_EST_exp_\", transform=transforms)\n",
    "data = NCaltech101ImageDataset(\"./Datasety/Ncaltech101_EST_trilinear_corr\", transform=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b566e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8709/8709 [1:20:31<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10675561., device='cuda:0')\n",
      "Mean MAE: tensor(0.0067, device='cuda:0')\n",
      "Mean MSE: tensor(0.0008, device='cuda:0')\n",
      "accuracy: 0.6217705821563899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.set_stage(1)\n",
    "spike_acc = 0\n",
    "MAE_sum = 0\n",
    "MSE_sum = 0\n",
    "TP = 0\n",
    "for i in tqdm(range(len(data))):#10\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            temp_data, temp_label = deepcopy(data[i])\n",
    "            temp_ttfs = (1 - temp_data).unsqueeze(0).to(\"cuda\")\n",
    "            temp_data = temp_data.to(\"cuda\")\n",
    "            # print(temp_data.shape)\n",
    "            # print(model_ttfs.forward(temp_ttfs))\n",
    "            out, spike_sum, v_sqare_sum = model_ttfs.forward(temp_ttfs)\n",
    "            spike_acc+=spike_sum\n",
    "            output = ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:]\n",
    "            output_true = model_resnet(temp_data.unsqueeze(0))\n",
    "            MAE = torch.sum((output-output_true).abs())/101.0\n",
    "            MSE = torch.sqrt(torch.sum((output-output_true)*(output-output_true)))\n",
    "            MAE_sum += MAE\n",
    "            MSE_sum += MSE\n",
    "            TP += (torch.argmax(temp_label)==torch.argmax(output)) \n",
    "            del temp_data, temp_ttfs\n",
    "    if i%100==0:\n",
    "        torch.cuda.empty_cache()\n",
    "print(spike_acc/len(data))\n",
    "print(\"Mean MAE:\", MAE_sum/len(data))\n",
    "print(\"Mean MSE:\", MSE_sum/(len(data)*101.0))\n",
    "print(\"accuracy:\",float(TP)/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed5066f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5415, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8addfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91) tensor(67, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = 8000\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        temp_data, temp_label = deepcopy(data[i])\n",
    "        temp_ttfs = (1 - temp_data).unsqueeze(0).to(\"cuda\")\n",
    "        temp_data = temp_data.to(\"cuda\")\n",
    "        # print(temp_data.shape)\n",
    "        # print(model_ttfs.forward(temp_ttfs))\n",
    "        out, spike_sum, v_sqare_sum = model_ttfs.forward(temp_ttfs)\n",
    "        spike_acc+=spike_sum\n",
    "        output = ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:]\n",
    "        output_true = model_resnet(temp_data.unsqueeze(0))\n",
    "        MAE = torch.sum((output-output_true).abs())/101.0\n",
    "        MSE = torch.sqrt(torch.sum((output-output_true)*(output-output_true)))\n",
    "        MAE_sum += MAE\n",
    "        MSE_sum += MSE\n",
    "        TP += (torch.argmax(temp_label)==torch.argmax(output)) \n",
    "        print(torch.argmax(temp_label), torch.argmax(output))\n",
    "        del temp_data, temp_ttfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
