{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a85a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4b715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=-1.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_chanels = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_chanels, 64, kernel_size = 7, stride = 1, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c8f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = ResNet(ResidualBlock, [5, 6, 6, 4], num_classes = 101, in_chanels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc3c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer0): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=101, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet=torch.load(\"best_resnet_nCaltech101_Hardtanh_ReLUmaxpool_EST_FC2_corrected_exp.pt\", weights_only=False)\n",
    "model_resnet.eval().to(\"cuda\")\n",
    "print(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e2765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4523,  2.3179,  2.4930,  0.5591,  1.1282, -1.4592,  1.9931, -0.3587,\n",
       "          3.3126, -2.9173, -0.2203, -0.3653, -5.7149,  1.3805,  1.7708, -0.2346,\n",
       "         -2.8358,  2.6006,  8.1917, -3.3833, -3.3626, -1.2714, -6.8086, -0.4964,\n",
       "         -1.7192, -3.0392, -0.6481,  1.1104, -3.0696, -2.1792,  4.4010,  1.1994,\n",
       "          5.2197, -3.1094, -5.6506,  3.6647, -2.8838, -2.4429,  1.1217, -4.0740,\n",
       "          0.9619, -1.6994, -3.1482,  3.3794,  1.3600,  2.0743,  5.3976, -4.1290,\n",
       "         -0.4036, -1.3789,  0.9460, -6.6034, -5.5881, -1.3458, -2.1289,  3.9662,\n",
       "         -4.7638, -5.3037, -1.5208, -1.0767,  7.7006,  2.9608, -0.4339,  0.7386,\n",
       "          5.2189, -1.5029,  3.4371,  2.6185, -1.4091,  5.1200, -0.1363,  2.5581,\n",
       "         -3.8921, -2.2180,  3.8134,  3.9212, -1.0427, -0.1211, -2.8506,  0.5521,\n",
       "         -0.2868, -0.9606, -4.0665, -0.6620,  3.5067,  8.7958, -0.6043, -4.4645,\n",
       "          1.0025, -1.3193, -3.8457, -6.7153,  2.4497,  4.1523,  1.1178, -3.2993,\n",
       "         -0.4880, -0.3057, -3.8100,  5.8504,  4.0221]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a879ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SNN.Check_Hardtanh import fuse_conv_and_bn\n",
    "from SNN.Check_Hardtanh import SpikingConv2D\n",
    "from SNN.Check_Hardtanh import MaxMinPool2D\n",
    "from SNN.Check_Hardtanh import LayerSNN_all\n",
    "from SNN.Check_Hardtanh import SpikingDense_positive_tanH, SpikingDense_all_all, SpikingDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74e5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_ttfs(nn.Module):\n",
    "    def __init__(self, model : ResNet):\n",
    "        super(ResNet_ttfs, self).__init__()\n",
    "        model.eval()\n",
    "        robustness_params={\n",
    "            'noise':0.0,\n",
    "            'time_bits':0,\n",
    "            'weight_bits': 0,\n",
    "            'latency_quantiles':0.0\n",
    "        }\n",
    "        model.to('cuda')\n",
    "        conv_fused = fuse_conv_and_bn(model.conv1[0], model.conv1[1], device = 'cuda')\n",
    "        self.conv_first = SpikingConv2D(64, \"temp1\", device = 'cuda', padding=(3,3), stride=2, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        max_vect = torch.tensor([1]*6)\n",
    "        tmin, tmax, max_vect, scalar = self.conv_first.set_params(0.0,1.0,max_vect)\n",
    "        self.tmin_post_pool = tmax\n",
    "        self.pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cuda\")\n",
    "        self.layer0SNN = LayerSNN_all(model.layer0, 64, 64, 5,device = 'cuda')\n",
    "        tmax_prev = tmax\n",
    "        tmin, tmax, max_vect, scalar = self.layer0SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer1SNN = LayerSNN_all(model.layer1, 64, 128, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer1SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer2SNN = LayerSNN_all(model.layer2, 128, 256, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer2SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer3SNN = LayerSNN_all(model.layer3, 256, 512, 4,device = 'cuda',end_maxpool=True)\n",
    "        tmin, tmax, max_vect, scalar = self.layer3SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cuda\")\n",
    "        # temp = torch.ones((512,2,2))\n",
    "        # max_vect = ((temp.T)*max_vect[:512]).T\n",
    "        # max_vect = max_vect.view(-1)\n",
    "        print(max_vect.shape)\n",
    "        max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect[:512]).T.contiguous().view(512).to(\"cuda\")\n",
    "\n",
    "        weights = model.fc.weight.detach().clone()\n",
    "        biases = model.fc.bias.detach().clone()\n",
    "        self.layer_fc = SpikingDense_positive_tanH(256, 512,\"test\",robustness_params=robustness_params,device = 'cuda',weights=weights, biases=biases)\n",
    "        \n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer_fc2 = SpikingDense_all_all(101,256, '',model.fc2.weight, model.fc2.bias,robustness_params=robustness_params,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc2.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.tmin, self.tmax = tmin, tmax\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        spike_sum, v_sqare_sum, not_spiked_sum = 0,0.0, 0\n",
    "\n",
    "        x, spike, v, not_spiked  = self.conv_first(tj)\n",
    "        spike_sum += spike\n",
    "        v_sqare_sum +=v\n",
    "        not_spiked_sum += not_spiked\n",
    "        x, spike = self.pool(x)\n",
    "\n",
    "        spike_sum += spike\n",
    "        # print(x.shape)\n",
    "        x = x\n",
    "        x, spike1, v1, not_spiked1 = self.layer0SNN(torch.concat((x, torch.ones(x.shape).to(\"cuda\") * self.tmin_post_pool),dim=1))\n",
    "        x, spike2, v2, not_spiked2 = self.layer1SNN(x)\n",
    "\n",
    "        x, spike3, v3, not_spiked3 = self.layer2SNN(x)\n",
    "\n",
    "        x, spike4, v4, not_spiked4 = self.layer3SNN(x)\n",
    "        x = x[:512]\n",
    "        # print(x[1].max(),x[1].min())\n",
    "        x, spike5 = self.pool2(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        x, spike6, v5, not_spiked5 = self.layer_fc(x)\n",
    "\n",
    "        x, spike7, v6, not_spiked6 = self.layer_fc2(x)\n",
    "\n",
    "        spike_sum += spike1+spike2+spike3+spike4+spike5+spike6+spike7\n",
    "        v_sqare_sum += v1+v2+v3+v4+v5+v6\n",
    "        not_spiked_sum += not_spiked1+not_spiked2+not_spiked3+not_spiked4+not_spiked5+not_spiked6\n",
    "        return x, spike_sum, v_sqare_sum, not_spiked_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a9880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:282: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V+eps_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:281: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:310: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:314: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:541: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:558: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:559: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:611: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:612: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:627: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:631: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:735: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:736: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:754: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:758: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:756: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_28764\\3812665932.py:32: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect[:512]).T.contiguous().view(512).to(\"cuda\")\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:160: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:195: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "model_ttfs = ResNet_ttfs(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ab7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13565625, device='cuda:0') tensor(5038962., device='cuda:0', grad_fn=<AddBackward0>) tensor(27457837, device='cuda:0')\n",
      "tensor([[5.3928, 2.3536, 2.5330, 0.8869, 1.1957, 0.0000, 1.7190, 0.0000, 3.3793,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.5237, 1.8430, 0.0000, 0.0000, 2.6268,\n",
      "         8.1474, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.1469, 0.0000, 0.0000, 4.1434, 1.2764, 5.3309, 0.0000, 0.0000, 3.4391,\n",
      "         0.0000, 0.0000, 1.0816, 0.0000, 0.9725, 0.0000, 0.0000, 3.0529, 1.1003,\n",
      "         2.0662, 5.5486, 0.0000, 0.0000, 0.0000, 1.0284, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 3.9251, 0.0000, 0.0000, 0.0000, 0.0000, 7.8858, 3.0595, 0.0000,\n",
      "         0.8173, 4.9288, 0.0000, 3.2169, 2.8061, 0.0000, 5.0966, 0.0911, 2.6196,\n",
      "         0.0000, 0.0000, 3.6047, 3.8367, 0.0000, 0.1377, 0.0000, 0.6752, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 3.6294, 8.5780, 0.0000, 0.0000, 1.0805, 0.0000,\n",
      "         0.0000, 0.0000, 2.1726, 3.9640, 1.0372, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         5.8080, 4.3244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6784, 0.0000,\n",
      "         0.5474, 0.0000, 3.2460, 0.3450, 0.3944, 5.7055, 0.0000, 0.0000, 0.1366,\n",
      "         2.9114, 0.0000, 0.0000, 3.3656, 3.0080, 1.3093, 6.9127, 0.4470, 1.5764,\n",
      "         2.8999, 0.7405, 0.0000, 2.6915, 2.2779, 0.0000, 0.0000, 0.0000, 3.1840,\n",
      "         5.8108, 0.0000, 2.7014, 2.8017, 0.0000, 4.1428, 0.0000, 1.7788, 3.0804,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 4.5564, 0.2841, 1.3257, 0.0000, 6.3258,\n",
      "         5.6013, 1.4563, 2.3695, 0.0000, 4.6524, 5.1882, 1.5901, 1.2160, 0.0000,\n",
      "         0.0000, 0.3916, 0.0000, 0.0000, 1.4387, 0.0000, 0.0000, 1.6894, 0.0000,\n",
      "         0.0000, 0.0000, 4.0062, 2.4211, 0.0000, 0.0000, 1.1420, 0.0000, 2.7535,\n",
      "         0.0000, 0.2693, 0.6752, 3.9986, 0.3988, 0.0000, 0.0000, 0.6066, 4.5180,\n",
      "         0.0000, 1.4184, 3.9503, 6.3675, 0.0000, 0.0000, 0.0000, 3.2542, 0.4108,\n",
      "         0.2923, 3.4934, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "temp_ttfs = 1 - temp\n",
    "# print(model_ttfs.forward(temp_ttfs))\n",
    "out, spike_sum, v_sqare_sum, not_spike = model_ttfs.forward(temp_ttfs.to(\"cuda\"))\n",
    "\n",
    "print(spike_sum, v_sqare_sum, not_spike)\n",
    "print((model_ttfs.tmax - out)*model_ttfs.scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a5e359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3985,  2.3491,  2.5349,  0.8885,  1.1952, -1.6798,  1.7158, -0.5487,\n",
       "          3.3793, -3.2467, -0.3426, -0.3958, -5.7061,  1.5273,  1.8409, -0.1372,\n",
       "         -2.9144,  2.6233,  8.1514, -3.3649, -3.0060, -1.3111, -6.9109, -0.4449,\n",
       "         -1.5781, -2.9028, -0.7408,  1.1453, -2.6868, -2.2820,  4.1407,  1.2776,\n",
       "          5.3277, -3.1857, -5.8135,  3.4380, -2.7007, -2.8015,  1.0776, -4.1426,\n",
       "          0.9698, -1.7744, -3.0817,  3.0507,  1.0978,  2.0646,  5.5488, -4.5535,\n",
       "         -0.2897, -1.3255,  1.0288, -6.3236, -5.6044, -1.4606, -2.3727,  3.9237,\n",
       "         -4.6522, -5.1915, -1.5874, -1.2165,  7.8829,  3.0603, -0.3881,  0.8178,\n",
       "          4.9319, -1.4403,  3.2193,  2.8049, -1.6862,  5.1011,  0.0924,  2.6192,\n",
       "         -4.0046, -2.4191,  3.6043,  3.8306, -1.1436,  0.1393, -2.7476,  0.6722,\n",
       "         -0.2687, -0.6752, -3.9948, -0.3971,  3.6331,  8.5768, -0.6064, -4.5187,\n",
       "          1.0824, -1.4147, -3.9462, -6.3654,  2.1742,  3.9611,  1.0387, -3.2545,\n",
       "         -0.4068, -0.2905, -3.4933,  5.8066,  4.3264]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea43df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "torch.manual_seed(19)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    # v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(60),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])\n",
    "\n",
    "\n",
    "class NCaltech101ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir_file, transform=None, target_transform=None):\n",
    "        self.images = np.load(img_dir_file + '_x.npy')\n",
    "        self.labels = np.load(img_dir_file + '_y.npy')\n",
    "        self.transform = transform\n",
    "        self.transform = transform\n",
    "        self.stage = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        label_temp = np.zeros((101,))\n",
    "        label_temp[label] = 1\n",
    "        if self.stage == 0:\n",
    "            return self.transform(torch.tensor(image)), torch.tensor(label_temp)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label_temp)\n",
    "    \n",
    "    def set_stage(self, stage):\n",
    "        self.stage = stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b195df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# training_data = NCarsImageDataset(\"./Datasety/nCars_train_EST_exp_\", transform=transforms)\n",
    "data = NCaltech101ImageDataset(\"./Datasety/Ncaltech101_EST_exp_corr\", transform=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b566e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8709/8709 [3:03:00<00:00,  1.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13535074., device='cuda:0')\n",
      "Mean MAE: tensor(0.0072, device='cuda:0')\n",
      "Mean MSE: tensor(0.0009, device='cuda:0')\n",
      "accuracy: 0.6795269261683317\n",
      "relative fraction:  tensor(0.1827, device='cuda:0')\n",
      "Sum V_square^2/treshold^2:  tensor(5021309.5000, device='cuda:0')\n",
      "Not spiked:  tensor(3158.1245, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.set_stage(1)\n",
    "spike_acc = 0\n",
    "not_spike_acc = 0\n",
    "v_sqare_acc = 0.0\n",
    "MAE_sum = 0\n",
    "MSE_sum = 0\n",
    "TP = 0\n",
    "for i in tqdm(range(len(data))):#10\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            temp_data, temp_label = deepcopy(data[i])\n",
    "            temp_ttfs = (1 - temp_data).unsqueeze(0).to(\"cuda\")\n",
    "            temp_data = temp_data.to(\"cuda\")\n",
    "            # print(temp_data.shape)\n",
    "            # print(model_ttfs.forward(temp_ttfs))\n",
    "            out, spike_sum, v_sqare_sum, not_spike = model_ttfs.forward(temp_ttfs)\n",
    "            spike_acc+=spike_sum\n",
    "            not_spike_acc += not_spike\n",
    "            v_sqare_acc+=v_sqare_sum\n",
    "            output = ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:]\n",
    "            output_true = model_resnet(temp_data.unsqueeze(0))\n",
    "            MAE = torch.sum((output-output_true).abs())/101.0\n",
    "            MSE = torch.sqrt(torch.sum((output-output_true)*(output-output_true)))/101.0\n",
    "            MAE_sum += MAE\n",
    "            MSE_sum += MSE\n",
    "            TP += (torch.argmax(temp_label)==torch.argmax(output))\n",
    "            del temp_data, temp_ttfs\n",
    "    if i%100==0:\n",
    "        torch.cuda.empty_cache()\n",
    "print(spike_acc/len(data))\n",
    "print(\"Mean MAE:\", MAE_sum/len(data))\n",
    "print(\"Mean MSE:\", MSE_sum/(len(data)))\n",
    "print(\"accuracy:\",float(TP)/len(data))\n",
    "print(\"relative fraction: \", v_sqare_acc/not_spike_acc)\n",
    "print(\"Sum V_square^2/treshold^2: \", v_sqare_acc/len(data))\n",
    "print(\"Not spiked: \", not_spike/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "917fab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spiked:  tensor(27490362., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Not spiked: \", not_spike_acc/len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
