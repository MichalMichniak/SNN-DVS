{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a85a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4b715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=-1.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_chanels = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_chanels, 64, kernel_size = 7, stride = 1, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c8f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = ResNet(ResidualBlock, [5, 6, 6, 4], num_classes = 101, in_chanels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc3c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer0): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=101, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet=torch.load(\"best_resnet_nCaltech101_Hardtanh_ReLUmaxpool_EST_FC2_corrected_trilinear_98.pt\", weights_only=False)\n",
    "model_resnet.eval().to(\"cuda\")\n",
    "print(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e2765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.4549, -0.7699, -0.6710, -0.4772,  1.7812, -1.9952,  0.3704,  2.0780,\n",
       "         -4.6632, -2.8911,  4.9264,  0.8885, -4.1747,  0.5351, -5.3044, -3.1034,\n",
       "          3.0183, -0.6036,  2.3824, -5.8221, -0.9069,  1.5042, -2.7596, -0.8619,\n",
       "         -1.9166,  3.4755,  4.1127, -2.6982, -1.6412, -0.7610,  3.1219,  0.9123,\n",
       "          1.8907, -0.9885, -0.0906, -0.9517, -1.8468, -1.8142,  3.5716, -0.9091,\n",
       "          0.1116, -1.3690,  0.7212, -3.0327, -5.8912,  1.2094, -0.4483,  4.2604,\n",
       "         -0.6632,  0.9809, -5.0322, -0.6131, -0.1402,  2.5098, -3.4294,  2.3544,\n",
       "          1.3008, -1.5584, -1.1215,  2.0692,  2.8084,  4.3361,  4.2195,  1.9735,\n",
       "          4.1779, -5.7576,  2.3493,  0.1823, -6.4078,  2.0071, -4.2471,  2.9470,\n",
       "          0.5222,  2.0050,  7.8277,  0.5656,  0.2018, -3.6788,  0.6687,  4.0763,\n",
       "          1.4433, -1.9025,  0.5337, -2.4090, -0.5783,  1.0867,  1.8979, -2.9545,\n",
       "          2.5140,  2.9677, -5.2192, -5.3157,  3.7737,  3.2428,  1.0753, -0.3422,\n",
       "         -7.1640,  1.3967, -2.4920, -0.5432,  1.9822]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a879ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SNN.Check_Hardtanh import fuse_conv_and_bn\n",
    "from SNN.Check_Hardtanh import SpikingConv2D\n",
    "from SNN.Check_Hardtanh import MaxMinPool2D\n",
    "from SNN.Check_Hardtanh import LayerSNN_all\n",
    "from SNN.Check_Hardtanh import SpikingDense_positive_tanH, SpikingDense_all_all, SpikingDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74e5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_ttfs(nn.Module):\n",
    "    def __init__(self, model : ResNet):\n",
    "        super(ResNet_ttfs, self).__init__()\n",
    "        model.eval()\n",
    "        robustness_params={\n",
    "            'noise':0.0,\n",
    "            'time_bits':0,\n",
    "            'weight_bits': 0,\n",
    "            'latency_quantiles':0.0\n",
    "        }\n",
    "        model.to('cuda')\n",
    "        conv_fused = fuse_conv_and_bn(model.conv1[0], model.conv1[1], device = 'cuda')\n",
    "        self.conv_first = SpikingConv2D(64, \"temp1\", device = 'cuda', padding=(3,3), stride=2, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        max_vect = torch.tensor([1]*6)\n",
    "        tmin, tmax, max_vect, scalar = self.conv_first.set_params(0.0,1.0,max_vect)\n",
    "        self.tmin_post_pool = tmax\n",
    "        self.pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cuda\")\n",
    "        self.layer0SNN = LayerSNN_all(model.layer0, 64, 64, 5,device = 'cuda')\n",
    "        tmax_prev = tmax\n",
    "        tmin, tmax, max_vect, scalar = self.layer0SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer1SNN = LayerSNN_all(model.layer1, 64, 128, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer1SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer2SNN = LayerSNN_all(model.layer2, 128, 256, 6,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer2SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer3SNN = LayerSNN_all(model.layer3, 256, 512, 4,device = 'cuda',end_maxpool=True)\n",
    "        tmin, tmax, max_vect, scalar = self.layer3SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cuda\")\n",
    "        # temp = torch.ones((512,2,2))\n",
    "        # max_vect = ((temp.T)*max_vect[:512]).T\n",
    "        # max_vect = max_vect.view(-1)\n",
    "        print(max_vect.shape)\n",
    "        max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect[:512]).T.contiguous().view(512).to(\"cuda\")\n",
    "\n",
    "        weights = model.fc.weight.detach().clone()\n",
    "        biases = model.fc.bias.detach().clone()\n",
    "        self.layer_fc = SpikingDense_positive_tanH(256, 512,\"test\",robustness_params=robustness_params,device = 'cuda',weights=weights, biases=biases)\n",
    "        \n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer_fc2 = SpikingDense_all_all(101,256, '',model.fc2.weight, model.fc2.bias,robustness_params=robustness_params,device = 'cuda')\n",
    "        tmin, tmax, max_vect, scalar = self.layer_fc2.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.tmin, self.tmax = tmin, tmax\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        spike_sum, v_sqare_sum, not_spiked_sum = 0,0.0, 0\n",
    "\n",
    "        x, spike, v, not_spiked  = self.conv_first(tj)\n",
    "        spike_sum += spike\n",
    "        v_sqare_sum +=v\n",
    "        not_spiked_sum += not_spiked\n",
    "        x, spike = self.pool(x)\n",
    "\n",
    "        spike_sum += spike\n",
    "        # print(x.shape)\n",
    "        x = x\n",
    "        x, spike1, v1, not_spiked1 = self.layer0SNN(torch.concat((x, torch.ones(x.shape).to(\"cuda\") * self.tmin_post_pool),dim=1))\n",
    "        x, spike2, v2, not_spiked2 = self.layer1SNN(x)\n",
    "\n",
    "        x, spike3, v3, not_spiked3 = self.layer2SNN(x)\n",
    "\n",
    "        x, spike4, v4, not_spiked4 = self.layer3SNN(x)\n",
    "        x = x[:512]\n",
    "        # print(x[1].max(),x[1].min())\n",
    "        x, spike5 = self.pool2(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        x, spike6, v5, not_spiked5 = self.layer_fc(x)\n",
    "\n",
    "        x, spike7, v6, not_spiked6 = self.layer_fc2(x)\n",
    "\n",
    "        spike_sum += spike1+spike2+spike3+spike4+spike5+spike6+spike7\n",
    "        v_sqare_sum += v1+v2+v3+v4+v5+v6\n",
    "        not_spiked_sum += not_spiked1+not_spiked2+not_spiked3+not_spiked4+not_spiked5+not_spiked6\n",
    "        return x, spike_sum, v_sqare_sum, not_spiked_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a9880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:282: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V+eps_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:281: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:310: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:314: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:541: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:558: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:559: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:611: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:612: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:627: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:631: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:735: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:736: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:754: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:758: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V+eps_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:756: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_11116\\3812665932.py:32: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  max_vect = (torch.ones([512, 1, 1]).to(\"cuda\").T*max_vect[:512]).T.contiguous().view(512).to(\"cuda\")\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:160: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:195: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max).to(self.device), torch.tensor([(1)]).to(self.device)))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False).to(self.device)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check_Hardtanh.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "model_ttfs = ResNet_ttfs(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ab7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14941075, device='cuda:0') tensor(8942255., device='cuda:0', grad_fn=<AddBackward0>) tensor(26280066, device='cuda:0')\n",
      "tensor([[7.6986, 0.0000, 0.0000, 0.0000, 1.7163, 0.0000, 0.3225, 2.0340, 0.0000,\n",
      "         0.0000, 4.8692, 0.9483, 0.0000, 0.6109, 0.0000, 0.0000, 3.0273, 0.0000,\n",
      "         2.6493, 0.0000, 0.0000, 1.4897, 0.0000, 0.0000, 0.0000, 3.4178, 4.0522,\n",
      "         0.0000, 0.0000, 0.0000, 3.1711, 1.0172, 1.9574, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 3.5280, 0.0000, 0.1639, 0.0000, 0.6493, 0.0000, 0.0000,\n",
      "         1.1974, 0.0000, 4.2195, 0.0000, 0.9133, 0.0000, 0.0000, 0.0000, 2.4274,\n",
      "         0.0000, 2.3014, 1.2065, 0.0000, 0.0000, 1.9559, 2.8606, 4.3958, 4.2156,\n",
      "         1.9866, 4.3325, 0.0000, 2.3967, 0.2875, 0.0000, 2.1941, 0.0000, 2.9444,\n",
      "         0.4490, 2.0685, 7.8869, 0.7269, 0.1466, 0.0000, 0.8007, 4.0575, 1.6267,\n",
      "         0.0000, 0.5855, 0.0000, 0.0000, 1.2420, 1.9181, 0.0000, 2.6100, 2.8989,\n",
      "         0.0000, 0.0000, 3.8951, 3.4303, 1.1025, 0.0000, 0.0000, 1.4140, 0.0000,\n",
      "         0.0000, 2.0781, 0.0000, 0.7015, 0.6655, 0.3900, 0.0000, 2.1361, 0.0000,\n",
      "         0.0000, 4.5803, 2.9823, 0.0000, 0.0000, 4.4140, 0.0000, 5.3436, 3.1787,\n",
      "         0.0000, 0.5137, 0.0000, 5.8567, 0.9636, 0.0000, 2.8189, 0.9468, 1.9506,\n",
      "         0.0000, 0.0000, 2.7959, 1.6047, 0.8390, 0.0000, 0.0000, 0.0000, 0.9799,\n",
      "         0.1313, 0.8927, 1.9377, 1.8160, 0.0000, 0.9818, 0.0000, 1.4466, 0.0000,\n",
      "         3.0767, 5.7652, 0.0000, 0.2779, 0.0000, 0.7092, 0.0000, 5.1735, 0.7125,\n",
      "         0.2372, 0.0000, 3.4092, 0.0000, 0.0000, 1.6876, 1.0968, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 5.7724, 0.0000, 0.0000, 6.5553, 0.0000,\n",
      "         4.3249, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.7552, 0.0000,\n",
      "         0.0000, 0.0000, 1.9387, 0.0000, 2.3230, 0.6425, 0.0000, 0.0000, 3.1073,\n",
      "         0.0000, 0.0000, 5.3081, 5.2726, 0.0000, 0.0000, 0.0000, 0.4169, 7.2295,\n",
      "         0.0000, 2.4701, 0.4173, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = torch.rand((1,6,224,224))\n",
    "temp_ttfs = 1 - temp\n",
    "# print(model_ttfs.forward(temp_ttfs))\n",
    "out, spike_sum, v_sqare_sum, not_spike = model_ttfs.forward(temp_ttfs.to(\"cuda\"))\n",
    "\n",
    "print(spike_sum, v_sqare_sum, not_spike)\n",
    "print((model_ttfs.tmax - out)*model_ttfs.scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a5e359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.6984, -0.7046, -0.6676, -0.3887,  1.7143, -2.1369,  0.3208,  2.0340,\n",
       "         -4.5775, -2.9802,  4.8676,  0.9474, -4.4138,  0.6135, -5.3428, -3.1808,\n",
       "          3.0216, -0.5119,  2.6507, -5.8562, -0.9628,  1.4884, -2.8162, -0.9480,\n",
       "         -1.9503,  3.4153,  4.0505, -2.7961, -1.6047, -0.8419,  3.1686,  1.0198,\n",
       "          1.9541, -0.9811, -0.1313, -0.8922, -1.9346, -1.8167,  3.5239, -0.9831,\n",
       "          0.1649, -1.4457,  0.6468, -3.0771, -5.7645,  1.1991, -0.2768,  4.2190,\n",
       "         -0.7108,  0.9107, -5.1726, -0.7129, -0.2378,  2.4241, -3.4076,  2.3002,\n",
       "          1.2069, -1.6894, -1.0953,  1.9545,  2.8572,  4.3972,  4.2141,  1.9859,\n",
       "          4.3343, -5.7731,  2.3984,  0.2910, -6.5573,  2.1953, -4.3225,  2.9450,\n",
       "          0.4492,  2.0707,  7.8855,  0.7273,  0.1436, -3.7532,  0.8031,  4.0542,\n",
       "          1.6277, -1.9374,  0.5892, -2.3227, -0.6422,  1.2416,  1.9213, -3.1069,\n",
       "          2.6065,  2.9017, -5.3033, -5.2664,  3.8975,  3.4310,  1.1021, -0.4162,\n",
       "         -7.2292,  1.4139, -2.4715, -0.4182,  2.0769]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet(temp.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea43df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "torch.manual_seed(19)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    # v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(60),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])\n",
    "\n",
    "\n",
    "class NCaltech101ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir_file, transform=None, target_transform=None):\n",
    "        self.images = np.load(img_dir_file + '_x.npy')\n",
    "        self.labels = np.load(img_dir_file + '_y.npy')\n",
    "        self.transform = transform\n",
    "        self.transform = transform\n",
    "        self.stage = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        label_temp = np.zeros((101,))\n",
    "        label_temp[label] = 1\n",
    "        if self.stage == 0:\n",
    "            return self.transform(torch.tensor(image)), torch.tensor(label_temp)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label_temp)\n",
    "    \n",
    "    def set_stage(self, stage):\n",
    "        self.stage = stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b195df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# training_data = NCarsImageDataset(\"./Datasety/nCars_train_EST_exp_\", transform=transforms)\n",
    "data = NCaltech101ImageDataset(\"./Datasety/Ncaltech101_EST_trilinear_corr\", transform=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b566e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8709 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8709/8709 [3:03:33<00:00,  1.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14838930., device='cuda:0')\n",
      "Mean MAE: tensor(0.0067, device='cuda:0')\n",
      "Mean MSE: tensor(0.0008, device='cuda:0')\n",
      "accuracy: 0.5911126420943851\n",
      "relative fraction:  tensor(0.3396, device='cuda:0')\n",
      "Sum V_square^2/treshold^2:  tensor(8955840., device='cuda:0')\n",
      "Not spiked:  tensor(3034.6699, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.set_stage(1)\n",
    "spike_acc = 0\n",
    "not_spike_acc = 0\n",
    "v_sqare_acc = 0.0\n",
    "MAE_sum = 0\n",
    "MSE_sum = 0\n",
    "TP = 0\n",
    "for i in tqdm(range(len(data))):#10\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            temp_data, temp_label = deepcopy(data[i])\n",
    "            temp_ttfs = (1 - temp_data).unsqueeze(0).to(\"cuda\")\n",
    "            temp_data = temp_data.to(\"cuda\")\n",
    "            # print(temp_data.shape)\n",
    "            # print(model_ttfs.forward(temp_ttfs))\n",
    "            out, spike_sum, v_sqare_sum, not_spike = model_ttfs.forward(temp_ttfs)\n",
    "            spike_acc+=spike_sum\n",
    "            not_spike_acc += not_spike\n",
    "            v_sqare_acc+=v_sqare_sum\n",
    "            output = ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,:101] - ((model_ttfs.tmax - out)*model_ttfs.scalar)[0,101:]\n",
    "            output_true = model_resnet(temp_data.unsqueeze(0))\n",
    "            MAE = torch.sum((output-output_true).abs())/101.0\n",
    "            MSE = torch.sqrt(torch.sum((output-output_true)*(output-output_true)))/101.0\n",
    "            MAE_sum += MAE\n",
    "            MSE_sum += MSE\n",
    "            TP += (torch.argmax(temp_label)==torch.argmax(output))\n",
    "            del temp_data, temp_ttfs\n",
    "    if i%100==0:\n",
    "        torch.cuda.empty_cache()\n",
    "print(spike_acc/len(data))\n",
    "print(\"Mean MAE:\", MAE_sum/len(data))\n",
    "print(\"Mean MSE:\", MSE_sum/(len(data)))\n",
    "print(\"accuracy:\",float(TP)/len(data))\n",
    "print(\"relative fraction: \", v_sqare_acc/not_spike_acc)\n",
    "print(\"Sum V_square^2/treshold^2: \", v_sqare_acc/len(data))\n",
    "print(\"Not spiked: \", not_spike/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "917fab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spiked:  tensor(26372302., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Not spiked: \", not_spike_acc/len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
